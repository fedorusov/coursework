{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('new_data/kickstarter.csv', index_col = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 169962 entries, 808893409 to 1492584511\n",
      "Data columns (total 25 columns):\n",
      "category                       169962 non-null object\n",
      "country                        169962 non-null object\n",
      "disable_communication          169962 non-null bool\n",
      "state                          169962 non-null object\n",
      "usd_type                       169962 non-null object\n",
      "name_words                     169962 non-null int64\n",
      "name_chars                     169962 non-null int64\n",
      "blurb_words                    169962 non-null int64\n",
      "blurb_chars                    169962 non-null int64\n",
      "sub_category                   162114 non-null object\n",
      "usd_goal                       169962 non-null float64\n",
      "creation_to_launch_days        169962 non-null int64\n",
      "campaign_days                  169962 non-null int64\n",
      "launch_day                     169962 non-null object\n",
      "deadline_day                   169962 non-null object\n",
      "launch_month                   169962 non-null object\n",
      "deadline_month                 169962 non-null object\n",
      "category_goal_mean             169962 non-null float64\n",
      "category_pledged_mean          169962 non-null float64\n",
      "sub_category_goal_mean         169962 non-null float64\n",
      "sub_category_pledged_mean      169962 non-null float64\n",
      "category_goal_median           169962 non-null float64\n",
      "category_pledged_median        169962 non-null float64\n",
      "sub_category_goal_median       169962 non-null float64\n",
      "sub_category_pledged_median    169962 non-null float64\n",
      "dtypes: bool(1), float64(9), int64(6), object(9)\n",
      "memory usage: 32.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['disable_communication'] = df['disable_communication'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['state'] = df['state'].replace({'failed': 0, 'successful': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(169962, 238)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>name_words</th>\n",
       "      <th>name_chars</th>\n",
       "      <th>blurb_words</th>\n",
       "      <th>blurb_chars</th>\n",
       "      <th>usd_goal</th>\n",
       "      <th>creation_to_launch_days</th>\n",
       "      <th>campaign_days</th>\n",
       "      <th>category_goal_mean</th>\n",
       "      <th>category_pledged_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>deadline_month_December</th>\n",
       "      <th>deadline_month_February</th>\n",
       "      <th>deadline_month_January</th>\n",
       "      <th>deadline_month_July</th>\n",
       "      <th>deadline_month_June</th>\n",
       "      <th>deadline_month_March</th>\n",
       "      <th>deadline_month_May</th>\n",
       "      <th>deadline_month_November</th>\n",
       "      <th>deadline_month_October</th>\n",
       "      <th>deadline_month_September</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>808893409</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>41</td>\n",
       "      <td>19</td>\n",
       "      <td>130</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>9752.782075</td>\n",
       "      <td>5288.76098</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1691985762</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>17</td>\n",
       "      <td>92</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>9752.782075</td>\n",
       "      <td>5288.76098</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280037487</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>15</td>\n",
       "      <td>96</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>9752.782075</td>\n",
       "      <td>5288.76098</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 238 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            state  name_words  name_chars  blurb_words  blurb_chars  usd_goal  \\\n",
       "id                                                                              \n",
       "808893409       1           7          41           19          130    3000.0   \n",
       "1691985762      1           6          29           17           92   40000.0   \n",
       "1280037487      1           4          31           15           96   10000.0   \n",
       "\n",
       "            creation_to_launch_days  campaign_days  category_goal_mean  \\\n",
       "id                                                                       \n",
       "808893409                         3              8         9752.782075   \n",
       "1691985762                        5             60         9752.782075   \n",
       "1280037487                        3             60         9752.782075   \n",
       "\n",
       "            category_pledged_mean  ...  deadline_month_December  \\\n",
       "id                                 ...                            \n",
       "808893409              5288.76098  ...                        0   \n",
       "1691985762             5288.76098  ...                        0   \n",
       "1280037487             5288.76098  ...                        0   \n",
       "\n",
       "            deadline_month_February  deadline_month_January  \\\n",
       "id                                                            \n",
       "808893409                         0                       1   \n",
       "1691985762                        0                       0   \n",
       "1280037487                        0                       1   \n",
       "\n",
       "            deadline_month_July  deadline_month_June  deadline_month_March  \\\n",
       "id                                                                           \n",
       "808893409                     0                    0                     0   \n",
       "1691985762                    0                    0                     0   \n",
       "1280037487                    0                    0                     0   \n",
       "\n",
       "            deadline_month_May  deadline_month_November  \\\n",
       "id                                                        \n",
       "808893409                    0                        0   \n",
       "1691985762                   0                        0   \n",
       "1280037487                   0                        0   \n",
       "\n",
       "            deadline_month_October  deadline_month_September  \n",
       "id                                                            \n",
       "808893409                        0                         0  \n",
       "1691985762                       0                         0  \n",
       "1280037487                       0                         0  \n",
       "\n",
       "[3 rows x 238 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw = df.drop(columns = 'state')\n",
    "y = df.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_words</th>\n",
       "      <th>name_chars</th>\n",
       "      <th>blurb_words</th>\n",
       "      <th>blurb_chars</th>\n",
       "      <th>creation_to_launch_days</th>\n",
       "      <th>campaign_days</th>\n",
       "      <th>category_art</th>\n",
       "      <th>category_comics</th>\n",
       "      <th>category_crafts</th>\n",
       "      <th>category_dance</th>\n",
       "      <th>...</th>\n",
       "      <th>deadline_month_December</th>\n",
       "      <th>deadline_month_February</th>\n",
       "      <th>deadline_month_January</th>\n",
       "      <th>deadline_month_July</th>\n",
       "      <th>deadline_month_June</th>\n",
       "      <th>deadline_month_March</th>\n",
       "      <th>deadline_month_May</th>\n",
       "      <th>deadline_month_November</th>\n",
       "      <th>deadline_month_October</th>\n",
       "      <th>deadline_month_September</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.482492</td>\n",
       "      <td>0.399351</td>\n",
       "      <td>0.040937</td>\n",
       "      <td>0.671676</td>\n",
       "      <td>-0.339139</td>\n",
       "      <td>-2.087165</td>\n",
       "      <td>-0.348843</td>\n",
       "      <td>-0.200889</td>\n",
       "      <td>-0.184498</td>\n",
       "      <td>-0.134794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.299399</td>\n",
       "      <td>-0.268903</td>\n",
       "      <td>4.034579</td>\n",
       "      <td>-0.326247</td>\n",
       "      <td>-0.315483</td>\n",
       "      <td>-0.318271</td>\n",
       "      <td>-0.321401</td>\n",
       "      <td>-0.299177</td>\n",
       "      <td>-0.293669</td>\n",
       "      <td>-0.288137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.112206</td>\n",
       "      <td>-0.365553</td>\n",
       "      <td>-0.348979</td>\n",
       "      <td>-0.720849</td>\n",
       "      <td>-0.323864</td>\n",
       "      <td>2.311867</td>\n",
       "      <td>-0.348843</td>\n",
       "      <td>-0.200889</td>\n",
       "      <td>-0.184498</td>\n",
       "      <td>-0.134794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.299399</td>\n",
       "      <td>-0.268903</td>\n",
       "      <td>-0.247857</td>\n",
       "      <td>-0.326247</td>\n",
       "      <td>-0.315483</td>\n",
       "      <td>-0.318271</td>\n",
       "      <td>-0.321401</td>\n",
       "      <td>-0.299177</td>\n",
       "      <td>-0.293669</td>\n",
       "      <td>-0.288137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.628365</td>\n",
       "      <td>-0.238069</td>\n",
       "      <td>-0.738894</td>\n",
       "      <td>-0.574267</td>\n",
       "      <td>-0.339139</td>\n",
       "      <td>2.311867</td>\n",
       "      <td>-0.348843</td>\n",
       "      <td>-0.200889</td>\n",
       "      <td>-0.184498</td>\n",
       "      <td>-0.134794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.299399</td>\n",
       "      <td>-0.268903</td>\n",
       "      <td>4.034579</td>\n",
       "      <td>-0.326247</td>\n",
       "      <td>-0.315483</td>\n",
       "      <td>-0.318271</td>\n",
       "      <td>-0.321401</td>\n",
       "      <td>-0.299177</td>\n",
       "      <td>-0.293669</td>\n",
       "      <td>-0.288137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.223064</td>\n",
       "      <td>0.590577</td>\n",
       "      <td>0.625810</td>\n",
       "      <td>-0.061232</td>\n",
       "      <td>0.164926</td>\n",
       "      <td>-0.987407</td>\n",
       "      <td>-0.348843</td>\n",
       "      <td>-0.200889</td>\n",
       "      <td>-0.184498</td>\n",
       "      <td>-0.134794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.299399</td>\n",
       "      <td>-0.268903</td>\n",
       "      <td>-0.247857</td>\n",
       "      <td>3.065164</td>\n",
       "      <td>-0.315483</td>\n",
       "      <td>-0.318271</td>\n",
       "      <td>-0.321401</td>\n",
       "      <td>-0.299177</td>\n",
       "      <td>-0.293669</td>\n",
       "      <td>-0.288137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.223064</td>\n",
       "      <td>1.546707</td>\n",
       "      <td>-0.154021</td>\n",
       "      <td>0.158641</td>\n",
       "      <td>0.111464</td>\n",
       "      <td>0.366141</td>\n",
       "      <td>-0.348843</td>\n",
       "      <td>-0.200889</td>\n",
       "      <td>-0.184498</td>\n",
       "      <td>-0.134794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.299399</td>\n",
       "      <td>-0.268903</td>\n",
       "      <td>-0.247857</td>\n",
       "      <td>-0.326247</td>\n",
       "      <td>3.169740</td>\n",
       "      <td>-0.318271</td>\n",
       "      <td>-0.321401</td>\n",
       "      <td>-0.299177</td>\n",
       "      <td>-0.293669</td>\n",
       "      <td>-0.288137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 228 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   name_words  name_chars  blurb_words  blurb_chars  creation_to_launch_days  \\\n",
       "0    0.482492    0.399351     0.040937     0.671676                -0.339139   \n",
       "1    0.112206   -0.365553    -0.348979    -0.720849                -0.323864   \n",
       "2   -0.628365   -0.238069    -0.738894    -0.574267                -0.339139   \n",
       "3    1.223064    0.590577     0.625810    -0.061232                 0.164926   \n",
       "4    1.223064    1.546707    -0.154021     0.158641                 0.111464   \n",
       "\n",
       "   campaign_days  category_art  category_comics  category_crafts  \\\n",
       "0      -2.087165     -0.348843        -0.200889        -0.184498   \n",
       "1       2.311867     -0.348843        -0.200889        -0.184498   \n",
       "2       2.311867     -0.348843        -0.200889        -0.184498   \n",
       "3      -0.987407     -0.348843        -0.200889        -0.184498   \n",
       "4       0.366141     -0.348843        -0.200889        -0.184498   \n",
       "\n",
       "   category_dance  ...  deadline_month_December  deadline_month_February  \\\n",
       "0       -0.134794  ...                -0.299399                -0.268903   \n",
       "1       -0.134794  ...                -0.299399                -0.268903   \n",
       "2       -0.134794  ...                -0.299399                -0.268903   \n",
       "3       -0.134794  ...                -0.299399                -0.268903   \n",
       "4       -0.134794  ...                -0.299399                -0.268903   \n",
       "\n",
       "   deadline_month_January  deadline_month_July  deadline_month_June  \\\n",
       "0                4.034579            -0.326247            -0.315483   \n",
       "1               -0.247857            -0.326247            -0.315483   \n",
       "2                4.034579            -0.326247            -0.315483   \n",
       "3               -0.247857             3.065164            -0.315483   \n",
       "4               -0.247857            -0.326247             3.169740   \n",
       "\n",
       "   deadline_month_March  deadline_month_May  deadline_month_November  \\\n",
       "0             -0.318271           -0.321401                -0.299177   \n",
       "1             -0.318271           -0.321401                -0.299177   \n",
       "2             -0.318271           -0.321401                -0.299177   \n",
       "3             -0.318271           -0.321401                -0.299177   \n",
       "4             -0.318271           -0.321401                -0.299177   \n",
       "\n",
       "   deadline_month_October  deadline_month_September  \n",
       "0               -0.293669                 -0.288137  \n",
       "1               -0.293669                 -0.288137  \n",
       "2               -0.293669                 -0.288137  \n",
       "3               -0.293669                 -0.288137  \n",
       "4               -0.293669                 -0.288137  \n",
       "\n",
       "[5 rows x 228 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "cols_to_not_scale = [\n",
    "    'usd_goal', \n",
    "    'category_goal_mean',\n",
    "    'category_pledged_mean',\n",
    "    'sub_category_goal_mean',\n",
    "    'sub_category_pledged_mean',\n",
    "    'category_goal_median',\n",
    "    'category_pledged_median',\n",
    "    'sub_category_goal_median',\n",
    "    'sub_category_pledged_median'\n",
    "]\n",
    "cols_to_scale = [col for col in list(X_raw.columns) if col not in cols_to_not_scale]\n",
    "X = pd.DataFrame(scaler.fit_transform(X_raw.drop(columns = cols_to_not_scale)), columns=cols_to_scale)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(pd.DataFrame(X_raw.usd_goal))\n",
    "X_money = pd.DataFrame(scaler.transform(X_raw[cols_to_not_scale]), columns=cols_to_not_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>usd_goal</th>\n",
       "      <th>category_goal_mean</th>\n",
       "      <th>category_pledged_mean</th>\n",
       "      <th>sub_category_goal_mean</th>\n",
       "      <th>sub_category_pledged_mean</th>\n",
       "      <th>category_goal_median</th>\n",
       "      <th>category_pledged_median</th>\n",
       "      <th>sub_category_goal_median</th>\n",
       "      <th>sub_category_pledged_median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.034286</td>\n",
       "      <td>-0.028193</td>\n",
       "      <td>-0.032221</td>\n",
       "      <td>-0.029038</td>\n",
       "      <td>-0.032558</td>\n",
       "      <td>-0.033622</td>\n",
       "      <td>-0.034867</td>\n",
       "      <td>-0.032481</td>\n",
       "      <td>-0.034513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.000898</td>\n",
       "      <td>-0.028193</td>\n",
       "      <td>-0.032221</td>\n",
       "      <td>-0.029038</td>\n",
       "      <td>-0.032558</td>\n",
       "      <td>-0.033622</td>\n",
       "      <td>-0.034867</td>\n",
       "      <td>-0.032481</td>\n",
       "      <td>-0.034513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.027969</td>\n",
       "      <td>-0.028193</td>\n",
       "      <td>-0.032221</td>\n",
       "      <td>-0.029038</td>\n",
       "      <td>-0.032558</td>\n",
       "      <td>-0.033622</td>\n",
       "      <td>-0.034867</td>\n",
       "      <td>-0.032481</td>\n",
       "      <td>-0.034513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.031579</td>\n",
       "      <td>-0.028193</td>\n",
       "      <td>-0.032221</td>\n",
       "      <td>-0.029038</td>\n",
       "      <td>-0.032558</td>\n",
       "      <td>-0.033622</td>\n",
       "      <td>-0.034867</td>\n",
       "      <td>-0.032481</td>\n",
       "      <td>-0.034513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.014434</td>\n",
       "      <td>-0.028193</td>\n",
       "      <td>-0.032221</td>\n",
       "      <td>-0.029038</td>\n",
       "      <td>-0.032558</td>\n",
       "      <td>-0.033622</td>\n",
       "      <td>-0.034867</td>\n",
       "      <td>-0.032481</td>\n",
       "      <td>-0.034513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   usd_goal  category_goal_mean  category_pledged_mean  \\\n",
       "0 -0.034286           -0.028193              -0.032221   \n",
       "1 -0.000898           -0.028193              -0.032221   \n",
       "2 -0.027969           -0.028193              -0.032221   \n",
       "3 -0.031579           -0.028193              -0.032221   \n",
       "4 -0.014434           -0.028193              -0.032221   \n",
       "\n",
       "   sub_category_goal_mean  sub_category_pledged_mean  category_goal_median  \\\n",
       "0               -0.029038                  -0.032558             -0.033622   \n",
       "1               -0.029038                  -0.032558             -0.033622   \n",
       "2               -0.029038                  -0.032558             -0.033622   \n",
       "3               -0.029038                  -0.032558             -0.033622   \n",
       "4               -0.029038                  -0.032558             -0.033622   \n",
       "\n",
       "   category_pledged_median  sub_category_goal_median  \\\n",
       "0                -0.034867                 -0.032481   \n",
       "1                -0.034867                 -0.032481   \n",
       "2                -0.034867                 -0.032481   \n",
       "3                -0.034867                 -0.032481   \n",
       "4                -0.034867                 -0.032481   \n",
       "\n",
       "   sub_category_pledged_median  \n",
       "0                    -0.034513  \n",
       "1                    -0.034513  \n",
       "2                    -0.034513  \n",
       "3                    -0.034513  \n",
       "4                    -0.034513  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_money.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.join(X_money)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(169962, 237)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fedor/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.78      0.75     21965\n",
      "           1       0.82      0.77      0.80     29024\n",
      "\n",
      "    accuracy                           0.78     50989\n",
      "   macro avg       0.77      0.78      0.77     50989\n",
      "weighted avg       0.78      0.78      0.78     50989\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 118973 entries, 23008 to 121958\n",
      "Columns: 237 entries, name_words to sub_category_pledged_median\n",
      "dtypes: float64(237)\n",
      "memory usage: 216.0 MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from tpot import TPOTClassifier\\ntpot = TPOTClassifier(generations=5, population_size=20, verbosity=3, n_jobs = 8)\\ntpot.fit(X_train, y_train)\\nprint(tpot.score(X_test, y_test))\\nprint(classification_report(y_test, tpot.predict(X_test)))'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from tpot import TPOTClassifier\n",
    "tpot = TPOTClassifier(generations=5, population_size=20, verbosity=3, n_jobs = 8)\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))\n",
    "print(classification_report(y_test, tpot.predict(X_test)))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import autosklearn.classification\\nautoml = autosklearn.classification.AutoSklearnClassifier(include_estimators=[\"random_forest\", ], include_preprocessors=[\"no_preprocessing\", ])\\nautoml.fit(X_train, y_train, X_test, y_test)\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import autosklearn.classification\n",
    "automl = autosklearn.classification.AutoSklearnClassifier(include_estimators=[\"random_forest\", ], include_preprocessors=[\"no_preprocessing\", ])\n",
    "automl.fit(X_train, y_train, X_test, y_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'predictions = automl.predict(X_test)'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''predictions = automl.predict(X_test)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(classification_report(y_test, predictions))'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''print(classification_report(y_test, predictions))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['name_words', 'name_chars', 'blurb_words', 'blurb_chars',\n",
    "       'usd_goal', 'creation_to_launch_days', 'campaign_days',\n",
    "       'category_goal_mean',\n",
    "       'category_pledged_mean',\n",
    "       'sub_category_goal_mean',\n",
    "       'sub_category_pledged_mean',\n",
    "       'category_goal_median',\n",
    "       'category_pledged_median',\n",
    "       'sub_category_goal_median',\n",
    "       'sub_category_pledged_median']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1152 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[cols].hist(figsize=(16, 16));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols:\n",
    "    df_log[col] = df_log[col].astype('float64').add(1)\n",
    "    df_log[col] = np.log(df_log[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw = df_log.drop(columns = 'state')\n",
    "y = df_log.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_words</th>\n",
       "      <th>name_chars</th>\n",
       "      <th>blurb_words</th>\n",
       "      <th>blurb_chars</th>\n",
       "      <th>usd_goal</th>\n",
       "      <th>creation_to_launch_days</th>\n",
       "      <th>campaign_days</th>\n",
       "      <th>category_goal_mean</th>\n",
       "      <th>category_pledged_mean</th>\n",
       "      <th>sub_category_goal_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>deadline_month_December</th>\n",
       "      <th>deadline_month_February</th>\n",
       "      <th>deadline_month_January</th>\n",
       "      <th>deadline_month_July</th>\n",
       "      <th>deadline_month_June</th>\n",
       "      <th>deadline_month_March</th>\n",
       "      <th>deadline_month_May</th>\n",
       "      <th>deadline_month_November</th>\n",
       "      <th>deadline_month_October</th>\n",
       "      <th>deadline_month_September</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>808893409</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>3.737670</td>\n",
       "      <td>2.995732</td>\n",
       "      <td>4.875197</td>\n",
       "      <td>8.006701</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>9.18541</td>\n",
       "      <td>8.573528</td>\n",
       "      <td>9.084434</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1691985762</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>3.401197</td>\n",
       "      <td>2.890372</td>\n",
       "      <td>4.532599</td>\n",
       "      <td>10.596660</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>4.110874</td>\n",
       "      <td>9.18541</td>\n",
       "      <td>8.573528</td>\n",
       "      <td>9.084434</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280037487</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>3.465736</td>\n",
       "      <td>2.772589</td>\n",
       "      <td>4.574711</td>\n",
       "      <td>9.210440</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>4.110874</td>\n",
       "      <td>9.18541</td>\n",
       "      <td>8.573528</td>\n",
       "      <td>9.084434</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1469195434</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>3.806662</td>\n",
       "      <td>3.135494</td>\n",
       "      <td>4.709530</td>\n",
       "      <td>8.699681</td>\n",
       "      <td>4.248495</td>\n",
       "      <td>3.091042</td>\n",
       "      <td>9.18541</td>\n",
       "      <td>8.573528</td>\n",
       "      <td>9.084434</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>781581087</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>4.094345</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>4.762174</td>\n",
       "      <td>10.126671</td>\n",
       "      <td>4.143135</td>\n",
       "      <td>3.637586</td>\n",
       "      <td>9.18541</td>\n",
       "      <td>8.573528</td>\n",
       "      <td>9.084434</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 237 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            name_words  name_chars  blurb_words  blurb_chars   usd_goal  \\\n",
       "id                                                                        \n",
       "808893409     2.079442    3.737670     2.995732     4.875197   8.006701   \n",
       "1691985762    1.945910    3.401197     2.890372     4.532599  10.596660   \n",
       "1280037487    1.609438    3.465736     2.772589     4.574711   9.210440   \n",
       "1469195434    2.302585    3.806662     3.135494     4.709530   8.699681   \n",
       "781581087     2.302585    4.094345     2.944439     4.762174  10.126671   \n",
       "\n",
       "            creation_to_launch_days  campaign_days  category_goal_mean  \\\n",
       "id                                                                       \n",
       "808893409                  1.386294       2.197225             9.18541   \n",
       "1691985762                 1.791759       4.110874             9.18541   \n",
       "1280037487                 1.386294       4.110874             9.18541   \n",
       "1469195434                 4.248495       3.091042             9.18541   \n",
       "781581087                  4.143135       3.637586             9.18541   \n",
       "\n",
       "            category_pledged_mean  sub_category_goal_mean  ...  \\\n",
       "id                                                         ...   \n",
       "808893409                8.573528                9.084434  ...   \n",
       "1691985762               8.573528                9.084434  ...   \n",
       "1280037487               8.573528                9.084434  ...   \n",
       "1469195434               8.573528                9.084434  ...   \n",
       "781581087                8.573528                9.084434  ...   \n",
       "\n",
       "            deadline_month_December  deadline_month_February  \\\n",
       "id                                                             \n",
       "808893409                         0                        0   \n",
       "1691985762                        0                        0   \n",
       "1280037487                        0                        0   \n",
       "1469195434                        0                        0   \n",
       "781581087                         0                        0   \n",
       "\n",
       "            deadline_month_January  deadline_month_July  deadline_month_June  \\\n",
       "id                                                                             \n",
       "808893409                        1                    0                    0   \n",
       "1691985762                       0                    0                    0   \n",
       "1280037487                       1                    0                    0   \n",
       "1469195434                       0                    1                    0   \n",
       "781581087                        0                    0                    1   \n",
       "\n",
       "            deadline_month_March  deadline_month_May  deadline_month_November  \\\n",
       "id                                                                              \n",
       "808893409                      0                   0                        0   \n",
       "1691985762                     0                   0                        0   \n",
       "1280037487                     0                   0                        0   \n",
       "1469195434                     0                   0                        0   \n",
       "781581087                      0                   0                        0   \n",
       "\n",
       "            deadline_month_October  deadline_month_September  \n",
       "id                                                            \n",
       "808893409                        0                         0  \n",
       "1691985762                       0                         0  \n",
       "1280037487                       0                         0  \n",
       "1469195434                       0                         0  \n",
       "781581087                        0                         0  \n",
       "\n",
       "[5 rows x 237 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_words</th>\n",
       "      <th>name_chars</th>\n",
       "      <th>blurb_words</th>\n",
       "      <th>blurb_chars</th>\n",
       "      <th>creation_to_launch_days</th>\n",
       "      <th>campaign_days</th>\n",
       "      <th>category_art</th>\n",
       "      <th>category_comics</th>\n",
       "      <th>category_crafts</th>\n",
       "      <th>category_dance</th>\n",
       "      <th>...</th>\n",
       "      <th>deadline_month_December</th>\n",
       "      <th>deadline_month_February</th>\n",
       "      <th>deadline_month_January</th>\n",
       "      <th>deadline_month_July</th>\n",
       "      <th>deadline_month_June</th>\n",
       "      <th>deadline_month_March</th>\n",
       "      <th>deadline_month_May</th>\n",
       "      <th>deadline_month_November</th>\n",
       "      <th>deadline_month_October</th>\n",
       "      <th>deadline_month_September</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.598653</td>\n",
       "      <td>0.532721</td>\n",
       "      <td>0.169009</td>\n",
       "      <td>0.573659</td>\n",
       "      <td>-0.737985</td>\n",
       "      <td>-3.352950</td>\n",
       "      <td>-0.348843</td>\n",
       "      <td>-0.200889</td>\n",
       "      <td>-0.184498</td>\n",
       "      <td>-0.134794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.299399</td>\n",
       "      <td>-0.268903</td>\n",
       "      <td>4.034579</td>\n",
       "      <td>-0.326247</td>\n",
       "      <td>-0.315483</td>\n",
       "      <td>-0.318271</td>\n",
       "      <td>-0.321401</td>\n",
       "      <td>-0.299177</td>\n",
       "      <td>-0.293669</td>\n",
       "      <td>-0.288137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.303540</td>\n",
       "      <td>-0.101087</td>\n",
       "      <td>-0.143926</td>\n",
       "      <td>-0.430338</td>\n",
       "      <td>-0.481273</td>\n",
       "      <td>1.756809</td>\n",
       "      <td>-0.348843</td>\n",
       "      <td>-0.200889</td>\n",
       "      <td>-0.184498</td>\n",
       "      <td>-0.134794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.299399</td>\n",
       "      <td>-0.268903</td>\n",
       "      <td>-0.247857</td>\n",
       "      <td>-0.326247</td>\n",
       "      <td>-0.315483</td>\n",
       "      <td>-0.318271</td>\n",
       "      <td>-0.321401</td>\n",
       "      <td>-0.299177</td>\n",
       "      <td>-0.293669</td>\n",
       "      <td>-0.288137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.440084</td>\n",
       "      <td>0.020483</td>\n",
       "      <td>-0.493757</td>\n",
       "      <td>-0.306928</td>\n",
       "      <td>-0.737985</td>\n",
       "      <td>1.756809</td>\n",
       "      <td>-0.348843</td>\n",
       "      <td>-0.200889</td>\n",
       "      <td>-0.184498</td>\n",
       "      <td>-0.134794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.299399</td>\n",
       "      <td>-0.268903</td>\n",
       "      <td>4.034579</td>\n",
       "      <td>-0.326247</td>\n",
       "      <td>-0.315483</td>\n",
       "      <td>-0.318271</td>\n",
       "      <td>-0.321401</td>\n",
       "      <td>-0.299177</td>\n",
       "      <td>-0.293669</td>\n",
       "      <td>-0.288137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.091814</td>\n",
       "      <td>0.662682</td>\n",
       "      <td>0.584121</td>\n",
       "      <td>0.088165</td>\n",
       "      <td>1.074161</td>\n",
       "      <td>-0.966309</td>\n",
       "      <td>-0.348843</td>\n",
       "      <td>-0.200889</td>\n",
       "      <td>-0.184498</td>\n",
       "      <td>-0.134794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.299399</td>\n",
       "      <td>-0.268903</td>\n",
       "      <td>-0.247857</td>\n",
       "      <td>3.065164</td>\n",
       "      <td>-0.315483</td>\n",
       "      <td>-0.318271</td>\n",
       "      <td>-0.321401</td>\n",
       "      <td>-0.299177</td>\n",
       "      <td>-0.293669</td>\n",
       "      <td>-0.288137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.091814</td>\n",
       "      <td>1.204585</td>\n",
       "      <td>0.016661</td>\n",
       "      <td>0.242439</td>\n",
       "      <td>1.007454</td>\n",
       "      <td>0.493053</td>\n",
       "      <td>-0.348843</td>\n",
       "      <td>-0.200889</td>\n",
       "      <td>-0.184498</td>\n",
       "      <td>-0.134794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.299399</td>\n",
       "      <td>-0.268903</td>\n",
       "      <td>-0.247857</td>\n",
       "      <td>-0.326247</td>\n",
       "      <td>3.169740</td>\n",
       "      <td>-0.318271</td>\n",
       "      <td>-0.321401</td>\n",
       "      <td>-0.299177</td>\n",
       "      <td>-0.293669</td>\n",
       "      <td>-0.288137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 228 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   name_words  name_chars  blurb_words  blurb_chars  creation_to_launch_days  \\\n",
       "0    0.598653    0.532721     0.169009     0.573659                -0.737985   \n",
       "1    0.303540   -0.101087    -0.143926    -0.430338                -0.481273   \n",
       "2   -0.440084    0.020483    -0.493757    -0.306928                -0.737985   \n",
       "3    1.091814    0.662682     0.584121     0.088165                 1.074161   \n",
       "4    1.091814    1.204585     0.016661     0.242439                 1.007454   \n",
       "\n",
       "   campaign_days  category_art  category_comics  category_crafts  \\\n",
       "0      -3.352950     -0.348843        -0.200889        -0.184498   \n",
       "1       1.756809     -0.348843        -0.200889        -0.184498   \n",
       "2       1.756809     -0.348843        -0.200889        -0.184498   \n",
       "3      -0.966309     -0.348843        -0.200889        -0.184498   \n",
       "4       0.493053     -0.348843        -0.200889        -0.184498   \n",
       "\n",
       "   category_dance  ...  deadline_month_December  deadline_month_February  \\\n",
       "0       -0.134794  ...                -0.299399                -0.268903   \n",
       "1       -0.134794  ...                -0.299399                -0.268903   \n",
       "2       -0.134794  ...                -0.299399                -0.268903   \n",
       "3       -0.134794  ...                -0.299399                -0.268903   \n",
       "4       -0.134794  ...                -0.299399                -0.268903   \n",
       "\n",
       "   deadline_month_January  deadline_month_July  deadline_month_June  \\\n",
       "0                4.034579            -0.326247            -0.315483   \n",
       "1               -0.247857            -0.326247            -0.315483   \n",
       "2                4.034579            -0.326247            -0.315483   \n",
       "3               -0.247857             3.065164            -0.315483   \n",
       "4               -0.247857            -0.326247             3.169740   \n",
       "\n",
       "   deadline_month_March  deadline_month_May  deadline_month_November  \\\n",
       "0             -0.318271           -0.321401                -0.299177   \n",
       "1             -0.318271           -0.321401                -0.299177   \n",
       "2             -0.318271           -0.321401                -0.299177   \n",
       "3             -0.318271           -0.321401                -0.299177   \n",
       "4             -0.318271           -0.321401                -0.299177   \n",
       "\n",
       "   deadline_month_October  deadline_month_September  \n",
       "0               -0.293669                 -0.288137  \n",
       "1               -0.293669                 -0.288137  \n",
       "2               -0.293669                 -0.288137  \n",
       "3               -0.293669                 -0.288137  \n",
       "4               -0.293669                 -0.288137  \n",
       "\n",
       "[5 rows x 228 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "cols_to_not_scale = [\n",
    "    'usd_goal', \n",
    "    'category_goal_mean',\n",
    "    'category_pledged_mean',\n",
    "    'sub_category_goal_mean',\n",
    "    'sub_category_pledged_mean',\n",
    "    'category_goal_median',\n",
    "    'category_pledged_median',\n",
    "    'sub_category_goal_median',\n",
    "    'sub_category_pledged_median'\n",
    "]\n",
    "cols_to_scale = [col for col in list(X_raw.columns) if col not in cols_to_not_scale]\n",
    "X = pd.DataFrame(scaler.fit_transform(X_raw.drop(columns = cols_to_not_scale)), columns=cols_to_scale)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(pd.DataFrame(X_raw.usd_goal))\n",
    "X_money = pd.DataFrame(scaler.transform(X_raw[cols_to_not_scale]), columns=cols_to_not_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>usd_goal</th>\n",
       "      <th>category_goal_mean</th>\n",
       "      <th>category_pledged_mean</th>\n",
       "      <th>sub_category_goal_mean</th>\n",
       "      <th>sub_category_pledged_mean</th>\n",
       "      <th>category_goal_median</th>\n",
       "      <th>category_pledged_median</th>\n",
       "      <th>sub_category_goal_median</th>\n",
       "      <th>sub_category_pledged_median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.223993</td>\n",
       "      <td>0.454487</td>\n",
       "      <td>0.10228</td>\n",
       "      <td>0.396363</td>\n",
       "      <td>0.060054</td>\n",
       "      <td>-0.097796</td>\n",
       "      <td>-0.363036</td>\n",
       "      <td>0.069968</td>\n",
       "      <td>-0.274258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.266819</td>\n",
       "      <td>0.454487</td>\n",
       "      <td>0.10228</td>\n",
       "      <td>0.396363</td>\n",
       "      <td>0.060054</td>\n",
       "      <td>-0.097796</td>\n",
       "      <td>-0.363036</td>\n",
       "      <td>0.069968</td>\n",
       "      <td>-0.274258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.468894</td>\n",
       "      <td>0.454487</td>\n",
       "      <td>0.10228</td>\n",
       "      <td>0.396363</td>\n",
       "      <td>0.060054</td>\n",
       "      <td>-0.097796</td>\n",
       "      <td>-0.363036</td>\n",
       "      <td>0.069968</td>\n",
       "      <td>-0.274258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.174895</td>\n",
       "      <td>0.454487</td>\n",
       "      <td>0.10228</td>\n",
       "      <td>0.396363</td>\n",
       "      <td>0.060054</td>\n",
       "      <td>-0.097796</td>\n",
       "      <td>-0.363036</td>\n",
       "      <td>0.069968</td>\n",
       "      <td>-0.274258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.996288</td>\n",
       "      <td>0.454487</td>\n",
       "      <td>0.10228</td>\n",
       "      <td>0.396363</td>\n",
       "      <td>0.060054</td>\n",
       "      <td>-0.097796</td>\n",
       "      <td>-0.363036</td>\n",
       "      <td>0.069968</td>\n",
       "      <td>-0.274258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   usd_goal  category_goal_mean  category_pledged_mean  \\\n",
       "0 -0.223993            0.454487                0.10228   \n",
       "1  1.266819            0.454487                0.10228   \n",
       "2  0.468894            0.454487                0.10228   \n",
       "3  0.174895            0.454487                0.10228   \n",
       "4  0.996288            0.454487                0.10228   \n",
       "\n",
       "   sub_category_goal_mean  sub_category_pledged_mean  category_goal_median  \\\n",
       "0                0.396363                   0.060054             -0.097796   \n",
       "1                0.396363                   0.060054             -0.097796   \n",
       "2                0.396363                   0.060054             -0.097796   \n",
       "3                0.396363                   0.060054             -0.097796   \n",
       "4                0.396363                   0.060054             -0.097796   \n",
       "\n",
       "   category_pledged_median  sub_category_goal_median  \\\n",
       "0                -0.363036                  0.069968   \n",
       "1                -0.363036                  0.069968   \n",
       "2                -0.363036                  0.069968   \n",
       "3                -0.363036                  0.069968   \n",
       "4                -0.363036                  0.069968   \n",
       "\n",
       "   sub_category_pledged_median  \n",
       "0                    -0.274258  \n",
       "1                    -0.274258  \n",
       "2                    -0.274258  \n",
       "3                    -0.274258  \n",
       "4                    -0.274258  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_money.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.join(X_money)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(169962, 237)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fedor/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.79      0.76     21965\n",
      "           1       0.83      0.79      0.81     29024\n",
      "\n",
      "    accuracy                           0.79     50989\n",
      "   macro avg       0.78      0.79      0.78     50989\n",
      "weighted avg       0.79      0.79      0.79     50989\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fedor/.local/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 400\n",
      "building tree 2 of 400\n",
      "building tree 3 of 400\n",
      "building tree 4 of 400\n",
      "building tree 5 of 400\n",
      "building tree 6 of 400\n",
      "building tree 7 of 400\n",
      "building tree 8 of 400\n",
      "building tree 9 of 400\n",
      "building tree 10 of 400\n",
      "building tree 11 of 400\n",
      "building tree 12 of 400\n",
      "building tree 13 of 400\n",
      "building tree 14 of 400\n",
      "building tree 15 of 400\n",
      "building tree 16 of 400\n",
      "building tree 17 of 400\n",
      "building tree 18 of 400\n",
      "building tree 19 of 400\n",
      "building tree 20 of 400\n",
      "building tree 21 of 400\n",
      "building tree 22 of 400\n",
      "building tree 23 of 400\n",
      "building tree 24 of 400\n",
      "building tree 25 of 400\n",
      "building tree 26 of 400\n",
      "building tree 27 of 400\n",
      "building tree 28 of 400\n",
      "building tree 29 of 400\n",
      "building tree 30 of 400\n",
      "building tree 31 of 400\n",
      "building tree 32 of 400\n",
      "building tree 33 of 400\n",
      "building tree 34 of 400\n",
      "building tree 35 of 400\n",
      "building tree 36 of 400\n",
      "building tree 37 of 400\n",
      "building tree 38 of 400\n",
      "building tree 39 of 400\n",
      "building tree 40 of 400\n",
      "building tree 41 of 400\n",
      "building tree 42 of 400\n",
      "building tree 43 of 400\n",
      "building tree 44 of 400\n",
      "building tree 45 of 400\n",
      "building tree 46 of 400\n",
      "building tree 47 of 400\n",
      "building tree 48 of 400\n",
      "building tree 49 of 400\n",
      "building tree 50 of 400\n",
      "building tree 51 of 400\n",
      "building tree 52 of 400\n",
      "building tree 53 of 400\n",
      "building tree 54 of 400\n",
      "building tree 55 of 400\n",
      "building tree 56 of 400\n",
      "building tree 57 of 400\n",
      "building tree 58 of 400\n",
      "building tree 59 of 400\n",
      "building tree 60 of 400\n",
      "building tree 61 of 400\n",
      "building tree 62 of 400\n",
      "building tree 63 of 400\n",
      "building tree 64 of 400\n",
      "building tree 65 of 400\n",
      "building tree 66 of 400\n",
      "building tree 67 of 400\n",
      "building tree 68 of 400\n",
      "building tree 69 of 400\n",
      "building tree 70 of 400\n",
      "building tree 71 of 400\n",
      "building tree 72 of 400\n",
      "building tree 73 of 400\n",
      "building tree 74 of 400\n",
      "building tree 75 of 400\n",
      "building tree 76 of 400\n",
      "building tree 77 of 400\n",
      "building tree 78 of 400\n",
      "building tree 79 of 400\n",
      "building tree 80 of 400\n",
      "building tree 81 of 400\n",
      "building tree 82 of 400\n",
      "building tree 83 of 400\n",
      "building tree 84 of 400\n",
      "building tree 85 of 400\n",
      "building tree 86 of 400\n",
      "building tree 87 of 400\n",
      "building tree 88 of 400\n",
      "building tree 89 of 400\n",
      "building tree 90 of 400\n",
      "building tree 91 of 400\n",
      "building tree 92 of 400\n",
      "building tree 93 of 400\n",
      "building tree 94 of 400\n",
      "building tree 95 of 400\n",
      "building tree 96 of 400\n",
      "building tree 97 of 400\n",
      "building tree 98 of 400\n",
      "building tree 99 of 400\n",
      "building tree 100 of 400\n",
      "building tree 101 of 400\n",
      "building tree 102 of 400\n",
      "building tree 103 of 400\n",
      "building tree 104 of 400\n",
      "building tree 105 of 400\n",
      "building tree 106 of 400\n",
      "building tree 107 of 400\n",
      "building tree 108 of 400\n",
      "building tree 109 of 400\n",
      "building tree 110 of 400\n",
      "building tree 111 of 400\n",
      "building tree 112 of 400\n",
      "building tree 113 of 400\n",
      "building tree 114 of 400\n",
      "building tree 115 of 400\n",
      "building tree 116 of 400\n",
      "building tree 117 of 400\n",
      "building tree 118 of 400\n",
      "building tree 119 of 400\n",
      "building tree 120 of 400\n",
      "building tree 121 of 400\n",
      "building tree 122 of 400\n",
      "building tree 123 of 400\n",
      "building tree 124 of 400\n",
      "building tree 125 of 400\n",
      "building tree 126 of 400\n",
      "building tree 127 of 400\n",
      "building tree 128 of 400\n",
      "building tree 129 of 400\n",
      "building tree 130 of 400\n",
      "building tree 131 of 400\n",
      "building tree 132 of 400\n",
      "building tree 133 of 400\n",
      "building tree 134 of 400\n",
      "building tree 135 of 400\n",
      "building tree 136 of 400\n",
      "building tree 137 of 400\n",
      "building tree 138 of 400\n",
      "building tree 139 of 400\n",
      "building tree 140 of 400\n",
      "building tree 141 of 400\n",
      "building tree 142 of 400\n",
      "building tree 143 of 400\n",
      "building tree 144 of 400\n",
      "building tree 145 of 400\n",
      "building tree 146 of 400\n",
      "building tree 147 of 400\n",
      "building tree 148 of 400\n",
      "building tree 149 of 400\n",
      "building tree 150 of 400\n",
      "building tree 151 of 400\n",
      "building tree 152 of 400\n",
      "building tree 153 of 400\n",
      "building tree 154 of 400\n",
      "building tree 155 of 400\n",
      "building tree 156 of 400\n",
      "building tree 157 of 400\n",
      "building tree 158 of 400\n",
      "building tree 159 of 400\n",
      "building tree 160 of 400\n",
      "building tree 161 of 400\n",
      "building tree 162 of 400\n",
      "building tree 163 of 400\n",
      "building tree 164 of 400\n",
      "building tree 165 of 400\n",
      "building tree 166 of 400\n",
      "building tree 167 of 400\n",
      "building tree 168 of 400\n",
      "building tree 169 of 400\n",
      "building tree 170 of 400\n",
      "building tree 171 of 400\n",
      "building tree 172 of 400\n",
      "building tree 173 of 400\n",
      "building tree 174 of 400\n",
      "building tree 175 of 400\n",
      "building tree 176 of 400\n",
      "building tree 177 of 400\n",
      "building tree 178 of 400\n",
      "building tree 179 of 400\n",
      "building tree 180 of 400\n",
      "building tree 181 of 400\n",
      "building tree 182 of 400\n",
      "building tree 183 of 400\n",
      "building tree 184 of 400\n",
      "building tree 185 of 400\n",
      "building tree 186 of 400\n",
      "building tree 187 of 400\n",
      "building tree 188 of 400\n",
      "building tree 189 of 400\n",
      "building tree 190 of 400\n",
      "building tree 191 of 400\n",
      "building tree 192 of 400\n",
      "building tree 193 of 400\n",
      "building tree 194 of 400\n",
      "building tree 195 of 400\n",
      "building tree 196 of 400\n",
      "building tree 197 of 400\n",
      "building tree 198 of 400\n",
      "building tree 199 of 400\n",
      "building tree 200 of 400\n",
      "building tree 201 of 400\n",
      "building tree 202 of 400\n",
      "building tree 203 of 400\n",
      "building tree 204 of 400\n",
      "building tree 205 of 400\n",
      "building tree 206 of 400\n",
      "building tree 207 of 400\n",
      "building tree 208 of 400\n",
      "building tree 209 of 400\n",
      "building tree 210 of 400\n",
      "building tree 211 of 400\n",
      "building tree 212 of 400\n",
      "building tree 213 of 400\n",
      "building tree 214 of 400\n",
      "building tree 215 of 400\n",
      "building tree 216 of 400\n",
      "building tree 217 of 400\n",
      "building tree 218 of 400\n",
      "building tree 219 of 400\n",
      "building tree 220 of 400\n",
      "building tree 221 of 400\n",
      "building tree 222 of 400\n",
      "building tree 223 of 400\n",
      "building tree 224 of 400\n",
      "building tree 225 of 400\n",
      "building tree 226 of 400\n",
      "building tree 227 of 400\n",
      "building tree 228 of 400\n",
      "building tree 229 of 400\n",
      "building tree 230 of 400\n",
      "building tree 231 of 400\n",
      "building tree 232 of 400\n",
      "building tree 233 of 400\n",
      "building tree 234 of 400\n",
      "building tree 235 of 400\n",
      "building tree 236 of 400\n",
      "building tree 237 of 400\n",
      "building tree 238 of 400\n",
      "building tree 239 of 400\n",
      "building tree 240 of 400\n",
      "building tree 241 of 400\n",
      "building tree 242 of 400\n",
      "building tree 243 of 400\n",
      "building tree 244 of 400\n",
      "building tree 245 of 400\n",
      "building tree 246 of 400\n",
      "building tree 247 of 400\n",
      "building tree 248 of 400\n",
      "building tree 249 of 400\n",
      "building tree 250 of 400\n",
      "building tree 251 of 400\n",
      "building tree 252 of 400\n",
      "building tree 253 of 400\n",
      "building tree 254 of 400\n",
      "building tree 255 of 400\n",
      "building tree 256 of 400\n",
      "building tree 257 of 400\n",
      "building tree 258 of 400\n",
      "building tree 259 of 400\n",
      "building tree 260 of 400\n",
      "building tree 261 of 400\n",
      "building tree 262 of 400\n",
      "building tree 263 of 400\n",
      "building tree 264 of 400\n",
      "building tree 265 of 400\n",
      "building tree 266 of 400\n",
      "building tree 267 of 400\n",
      "building tree 268 of 400\n",
      "building tree 269 of 400\n",
      "building tree 270 of 400\n",
      "building tree 271 of 400\n",
      "building tree 272 of 400\n",
      "building tree 273 of 400\n",
      "building tree 274 of 400\n",
      "building tree 275 of 400\n",
      "building tree 276 of 400\n",
      "building tree 277 of 400\n",
      "building tree 278 of 400\n",
      "building tree 279 of 400\n",
      "building tree 280 of 400\n",
      "building tree 281 of 400\n",
      "building tree 282 of 400\n",
      "building tree 283 of 400\n",
      "building tree 284 of 400\n",
      "building tree 285 of 400\n",
      "building tree 286 of 400\n",
      "building tree 287 of 400\n",
      "building tree 288 of 400\n",
      "building tree 289 of 400\n",
      "building tree 290 of 400\n",
      "building tree 291 of 400\n",
      "building tree 292 of 400\n",
      "building tree 293 of 400\n",
      "building tree 294 of 400\n",
      "building tree 295 of 400\n",
      "building tree 296 of 400\n",
      "building tree 297 of 400\n",
      "building tree 298 of 400\n",
      "building tree 299 of 400\n",
      "building tree 300 of 400\n",
      "building tree 301 of 400\n",
      "building tree 302 of 400\n",
      "building tree 303 of 400\n",
      "building tree 304 of 400\n",
      "building tree 305 of 400\n",
      "building tree 306 of 400\n",
      "building tree 307 of 400\n",
      "building tree 308 of 400\n",
      "building tree 309 of 400\n",
      "building tree 310 of 400\n",
      "building tree 311 of 400\n",
      "building tree 312 of 400\n",
      "building tree 313 of 400\n",
      "building tree 314 of 400\n",
      "building tree 315 of 400\n",
      "building tree 316 of 400\n",
      "building tree 317 of 400\n",
      "building tree 318 of 400\n",
      "building tree 319 of 400\n",
      "building tree 320 of 400\n",
      "building tree 321 of 400\n",
      "building tree 322 of 400\n",
      "building tree 323 of 400\n",
      "building tree 324 of 400\n",
      "building tree 325 of 400\n",
      "building tree 326 of 400\n",
      "building tree 327 of 400\n",
      "building tree 328 of 400\n",
      "building tree 329 of 400\n",
      "building tree 330 of 400\n",
      "building tree 331 of 400\n",
      "building tree 332 of 400\n",
      "building tree 333 of 400\n",
      "building tree 334 of 400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 335 of 400\n",
      "building tree 336 of 400\n",
      "building tree 337 of 400\n",
      "building tree 338 of 400\n",
      "building tree 339 of 400\n",
      "building tree 340 of 400\n",
      "building tree 341 of 400\n",
      "building tree 342 of 400\n",
      "building tree 343 of 400\n",
      "building tree 344 of 400\n",
      "building tree 345 of 400\n",
      "building tree 346 of 400\n",
      "building tree 347 of 400\n",
      "building tree 348 of 400\n",
      "building tree 349 of 400\n",
      "building tree 350 of 400\n",
      "building tree 351 of 400\n",
      "building tree 352 of 400\n",
      "building tree 353 of 400\n",
      "building tree 354 of 400\n",
      "building tree 355 of 400\n",
      "building tree 356 of 400\n",
      "building tree 357 of 400\n",
      "building tree 358 of 400\n",
      "building tree 359 of 400\n",
      "building tree 360 of 400\n",
      "building tree 361 of 400\n",
      "building tree 362 of 400\n",
      "building tree 363 of 400\n",
      "building tree 364 of 400\n",
      "building tree 365 of 400\n",
      "building tree 366 of 400\n",
      "building tree 367 of 400\n",
      "building tree 368 of 400\n",
      "building tree 369 of 400\n",
      "building tree 370 of 400\n",
      "building tree 371 of 400\n",
      "building tree 372 of 400\n",
      "building tree 373 of 400\n",
      "building tree 374 of 400\n",
      "building tree 375 of 400\n",
      "building tree 376 of 400\n",
      "building tree 377 of 400\n",
      "building tree 378 of 400\n",
      "building tree 379 of 400\n",
      "building tree 380 of 400\n",
      "building tree 381 of 400\n",
      "building tree 382 of 400\n",
      "building tree 383 of 400\n",
      "building tree 384 of 400\n",
      "building tree 385 of 400\n",
      "building tree 386 of 400\n",
      "building tree 387 of 400\n",
      "building tree 388 of 400\n",
      "building tree 389 of 400\n",
      "building tree 390 of 400\n",
      "building tree 391 of 400\n",
      "building tree 392 of 400\n",
      "building tree 393 of 400\n",
      "building tree 394 of 400\n",
      "building tree 395 of 400\n",
      "building tree 396 of 400\n",
      "building tree 397 of 400\n",
      "building tree 398 of 400\n",
      "building tree 399 of 400\n",
      "building tree 400 of 400\n",
      "0.794650887176082 {'max_depth': 35, 'min_samples_split': 10, 'n_estimators': 400}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:  1.1min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "rf = RandomForestClassifier(verbose=2)\n",
    "\n",
    "params = [ \n",
    "  {'n_estimators': [200, 400],\n",
    "   'max_depth': [20, 35],\n",
    "   'min_samples_split' : [10, 30, 100]\n",
    "  }\n",
    "]\n",
    "\n",
    "gs = GridSearchCV(rf, params, cv=5, n_jobs = 4)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(gs.best_score_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/fedor/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/fedor/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/fedor/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/fedor/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/fedor/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/fedor/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer(i, token, input_dim):\n",
    "    kwargs = {}\n",
    "    if i == 0:\n",
    "        kwargs['input_dim'] = input_dim\n",
    "    if token[:2] == 'DS':\n",
    "        return Dense(int(token[3:]), activation='sigmoid', **kwargs)\n",
    "    if token[:2] == 'DR':\n",
    "        return Dense(int(token[3:]), activation='relu', **kwargs)\n",
    "    if token[:2] == 'DL':\n",
    "        if token[2] == '_':\n",
    "            return Dense(int(token[3:]), activation=LeakyReLU, **kwargs)\n",
    "        else:\n",
    "            return Dense(int(token[3:]), activation=LeakyReLU(0.1*int(token[2])), **kwargs)\n",
    "    if token[:2] == 'BN':\n",
    "        return BatchNormalization()\n",
    "    if token[:2] == 'DO':\n",
    "        return Dropout(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_NN(loss, optimizer, layers, input_dim):\n",
    "    model = Sequential()\n",
    "    for i, token in enumerate(layers.split()):\n",
    "        model.add(get_layer(i, token, input_dim))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "    'DS_128 DS_64 BN DO DS_32 DS_8',\n",
    "    'DS_256 DS_128 BN DO DS_64 DS_32 BN DO DS_16 DS_8',\n",
    "    'DR_256 DR_512 BN DR_512 DR_256 BN DO DR_512 BN DO',\n",
    "    'DS_256 DS_512 BN DS_512 DS_256 BN DO DS_512 BN DO',\n",
    "    'DL1256 DL1512 BN DL1512 DL1256 BN DO DL1512 BN DO'\n",
    "]\n",
    "OPTIMIZERS = [\n",
    "    'RMSprop', 'Adam', 'Adadelta', 'Nadam', 'Adamax', 'Adagrad', 'SGD'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame([[0.7, 'DS_128 DS_64 BN DO DS_32 DS_8', 'RMSprop'], [0.9, 'kek', 'Winner']], columns = ['accuracy', 'layers', 'optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>layers</th>\n",
       "      <th>optimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>kek</td>\n",
       "      <td>Winner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>DS_128 DS_64 BN DO DS_32 DS_8</td>\n",
       "      <td>RMSprop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy                         layers optimizer\n",
       "1       0.9                            kek    Winner\n",
       "0       0.7  DS_128 DS_64 BN DO DS_32 DS_8   RMSprop"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values('accuracy', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network= DS_128 DS_64 BN DO DS_32 DS_8\n",
      "optimizer= RMSprop \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 2s 16us/step - loss: 0.4913 - accuracy: 0.7649 - val_loss: 0.4632 - val_accuracy: 0.7677\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.4402 - accuracy: 0.7804 - val_loss: 0.4313 - val_accuracy: 0.7792\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.4253 - accuracy: 0.7846 - val_loss: 0.4267 - val_accuracy: 0.7805\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.4186 - accuracy: 0.7861 - val_loss: 0.4223 - val_accuracy: 0.7825\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.4125 - accuracy: 0.7901 - val_loss: 0.4152 - val_accuracy: 0.7839\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.4082 - accuracy: 0.7942 - val_loss: 0.4125 - val_accuracy: 0.7864\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.4046 - accuracy: 0.7960 - val_loss: 0.4102 - val_accuracy: 0.7878\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.4016 - accuracy: 0.7976 - val_loss: 0.4132 - val_accuracy: 0.7872\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.3998 - accuracy: 0.7983 - val_loss: 0.4092 - val_accuracy: 0.7876\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.3982 - accuracy: 0.8002 - val_loss: 0.4081 - val_accuracy: 0.7895\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.3963 - accuracy: 0.8003 - val_loss: 0.4084 - val_accuracy: 0.7914\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.3944 - accuracy: 0.8019 - val_loss: 0.4095 - val_accuracy: 0.7924\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.3925 - accuracy: 0.8036 - val_loss: 0.4061 - val_accuracy: 0.7913\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.3904 - accuracy: 0.8031 - val_loss: 0.4074 - val_accuracy: 0.7909\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.3883 - accuracy: 0.8054 - val_loss: 0.4098 - val_accuracy: 0.7891\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.76      0.76     10216\n",
      "           1       0.82      0.81      0.81     13579\n",
      "\n",
      "    accuracy                           0.79     23795\n",
      "   macro avg       0.78      0.79      0.78     23795\n",
      "weighted avg       0.79      0.79      0.79     23795\n",
      "\n",
      "network= DS_128 DS_64 BN DO DS_32 DS_8\n",
      "optimizer= Adam \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 2s 17us/step - loss: 0.4747 - accuracy: 0.7670 - val_loss: 0.4494 - val_accuracy: 0.7747\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.4351 - accuracy: 0.7805 - val_loss: 0.4291 - val_accuracy: 0.7769\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.4211 - accuracy: 0.7824 - val_loss: 0.4185 - val_accuracy: 0.7808\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.4137 - accuracy: 0.7850 - val_loss: 0.4179 - val_accuracy: 0.7819\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 2s 16us/step - loss: 0.4081 - accuracy: 0.7889 - val_loss: 0.4124 - val_accuracy: 0.7848\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.4045 - accuracy: 0.7914 - val_loss: 0.4138 - val_accuracy: 0.7858\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.4005 - accuracy: 0.7932 - val_loss: 0.4087 - val_accuracy: 0.7866\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.3970 - accuracy: 0.7962 - val_loss: 0.4085 - val_accuracy: 0.7893\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.3927 - accuracy: 0.7990 - val_loss: 0.4081 - val_accuracy: 0.7910\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.3882 - accuracy: 0.8023 - val_loss: 0.4065 - val_accuracy: 0.7900\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.3845 - accuracy: 0.8051 - val_loss: 0.4075 - val_accuracy: 0.7884\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 1s 16us/step - loss: 0.3809 - accuracy: 0.8072 - val_loss: 0.4092 - val_accuracy: 0.7906\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.3762 - accuracy: 0.8109 - val_loss: 0.4095 - val_accuracy: 0.7891\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.3721 - accuracy: 0.8130 - val_loss: 0.4135 - val_accuracy: 0.7891\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.3667 - accuracy: 0.8168 - val_loss: 0.4139 - val_accuracy: 0.7861\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.73      0.74     10216\n",
      "           1       0.80      0.83      0.82     13579\n",
      "\n",
      "    accuracy                           0.79     23795\n",
      "   macro avg       0.78      0.78      0.78     23795\n",
      "weighted avg       0.79      0.79      0.79     23795\n",
      "\n",
      "network= DS_128 DS_64 BN DO DS_32 DS_8\n",
      "optimizer= Adadelta \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 2s 17us/step - loss: 0.4858 - accuracy: 0.7579 - val_loss: 0.4514 - val_accuracy: 0.7726\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.4411 - accuracy: 0.7788 - val_loss: 0.4344 - val_accuracy: 0.7758\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.4264 - accuracy: 0.7826 - val_loss: 0.4237 - val_accuracy: 0.7801\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.4176 - accuracy: 0.7836 - val_loss: 0.4216 - val_accuracy: 0.7794\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.4125 - accuracy: 0.7857 - val_loss: 0.4158 - val_accuracy: 0.7823\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.4094 - accuracy: 0.7884 - val_loss: 0.4127 - val_accuracy: 0.7845\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.4063 - accuracy: 0.7901 - val_loss: 0.4120 - val_accuracy: 0.7847\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.4033 - accuracy: 0.7921 - val_loss: 0.4108 - val_accuracy: 0.7865\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.4016 - accuracy: 0.7927 - val_loss: 0.4087 - val_accuracy: 0.7874\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 2s 16us/step - loss: 0.3989 - accuracy: 0.7953 - val_loss: 0.4089 - val_accuracy: 0.7883\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.3969 - accuracy: 0.7966 - val_loss: 0.4090 - val_accuracy: 0.7888\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.3947 - accuracy: 0.7985 - val_loss: 0.4083 - val_accuracy: 0.7876\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 2s 17us/step - loss: 0.3927 - accuracy: 0.8004 - val_loss: 0.4091 - val_accuracy: 0.7870\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.3910 - accuracy: 0.8003 - val_loss: 0.4103 - val_accuracy: 0.7884\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.3892 - accuracy: 0.8020 - val_loss: 0.4087 - val_accuracy: 0.7874\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.74      0.75     10216\n",
      "           1       0.81      0.82      0.81     13579\n",
      "\n",
      "    accuracy                           0.79     23795\n",
      "   macro avg       0.78      0.78      0.78     23795\n",
      "weighted avg       0.79      0.79      0.79     23795\n",
      "\n",
      "network= DS_128 DS_64 BN DO DS_32 DS_8\n",
      "optimizer= Nadam \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 2s 19us/step - loss: 0.4707 - accuracy: 0.7696 - val_loss: 0.4350 - val_accuracy: 0.7760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 2s 16us/step - loss: 0.4229 - accuracy: 0.7813 - val_loss: 0.4195 - val_accuracy: 0.7800\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.4114 - accuracy: 0.7871 - val_loss: 0.4157 - val_accuracy: 0.7851\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 1s 16us/step - loss: 0.4057 - accuracy: 0.7904 - val_loss: 0.4106 - val_accuracy: 0.7856\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 2s 18us/step - loss: 0.4007 - accuracy: 0.7942 - val_loss: 0.4080 - val_accuracy: 0.7885\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 1s 16us/step - loss: 0.3958 - accuracy: 0.7972 - val_loss: 0.4108 - val_accuracy: 0.7864\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.3893 - accuracy: 0.8004 - val_loss: 0.4052 - val_accuracy: 0.7891\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 2s 16us/step - loss: 0.3849 - accuracy: 0.8041 - val_loss: 0.4027 - val_accuracy: 0.7892\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 2s 16us/step - loss: 0.3781 - accuracy: 0.8082 - val_loss: 0.4110 - val_accuracy: 0.7871\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 2s 16us/step - loss: 0.3728 - accuracy: 0.8120 - val_loss: 0.4037 - val_accuracy: 0.7888\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 2s 16us/step - loss: 0.3660 - accuracy: 0.8164 - val_loss: 0.4118 - val_accuracy: 0.7873\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 2s 17us/step - loss: 0.3603 - accuracy: 0.8196 - val_loss: 0.4148 - val_accuracy: 0.7874\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 2s 16us/step - loss: 0.3531 - accuracy: 0.8235 - val_loss: 0.4186 - val_accuracy: 0.7812\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 1s 16us/step - loss: 0.3466 - accuracy: 0.8287 - val_loss: 0.4276 - val_accuracy: 0.7829\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 2s 16us/step - loss: 0.3392 - accuracy: 0.8339 - val_loss: 0.4277 - val_accuracy: 0.7841\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.77      0.75     10216\n",
      "           1       0.82      0.80      0.81     13579\n",
      "\n",
      "    accuracy                           0.78     23795\n",
      "   macro avg       0.78      0.78      0.78     23795\n",
      "weighted avg       0.79      0.78      0.78     23795\n",
      "\n",
      "network= DS_128 DS_64 BN DO DS_32 DS_8\n",
      "optimizer= Adamax \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 2s 18us/step - loss: 0.5010 - accuracy: 0.7689 - val_loss: 0.4621 - val_accuracy: 0.7770\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.4498 - accuracy: 0.7775 - val_loss: 0.4428 - val_accuracy: 0.7770\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.4341 - accuracy: 0.7812 - val_loss: 0.4345 - val_accuracy: 0.7797\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 2s 16us/step - loss: 0.4232 - accuracy: 0.7834 - val_loss: 0.4293 - val_accuracy: 0.7758\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 2s 16us/step - loss: 0.4156 - accuracy: 0.7865 - val_loss: 0.4179 - val_accuracy: 0.7841\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.4104 - accuracy: 0.7882 - val_loss: 0.4188 - val_accuracy: 0.7805\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.4066 - accuracy: 0.7917 - val_loss: 0.4124 - val_accuracy: 0.7859\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.4029 - accuracy: 0.7929 - val_loss: 0.4111 - val_accuracy: 0.7860\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 2s 16us/step - loss: 0.4001 - accuracy: 0.7955 - val_loss: 0.4084 - val_accuracy: 0.7877\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.3966 - accuracy: 0.7975 - val_loss: 0.4086 - val_accuracy: 0.7855\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.3950 - accuracy: 0.7987 - val_loss: 0.4085 - val_accuracy: 0.7883\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.3917 - accuracy: 0.8007 - val_loss: 0.4112 - val_accuracy: 0.7884\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.3900 - accuracy: 0.8023 - val_loss: 0.4074 - val_accuracy: 0.7874\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.3861 - accuracy: 0.8042 - val_loss: 0.4061 - val_accuracy: 0.7899\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.3844 - accuracy: 0.8043 - val_loss: 0.4086 - val_accuracy: 0.7889\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.73      0.75     10216\n",
      "           1       0.81      0.83      0.82     13579\n",
      "\n",
      "    accuracy                           0.79     23795\n",
      "   macro avg       0.79      0.78      0.78     23795\n",
      "weighted avg       0.79      0.79      0.79     23795\n",
      "\n",
      "network= DS_128 DS_64 BN DO DS_32 DS_8\n",
      "optimizer= Adagrad \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 2s 18us/step - loss: 0.4817 - accuracy: 0.7721 - val_loss: 0.4592 - val_accuracy: 0.7768\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.4483 - accuracy: 0.7819 - val_loss: 0.4469 - val_accuracy: 0.7791\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.4385 - accuracy: 0.7834 - val_loss: 0.4392 - val_accuracy: 0.7821\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.4320 - accuracy: 0.7844 - val_loss: 0.4341 - val_accuracy: 0.7824\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 2s 16us/step - loss: 0.4282 - accuracy: 0.7859 - val_loss: 0.4305 - val_accuracy: 0.7840\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.4234 - accuracy: 0.7875 - val_loss: 0.4269 - val_accuracy: 0.7820\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.4201 - accuracy: 0.7878 - val_loss: 0.4250 - val_accuracy: 0.7835\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.4180 - accuracy: 0.7889 - val_loss: 0.4225 - val_accuracy: 0.7833\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.4150 - accuracy: 0.7889 - val_loss: 0.4207 - val_accuracy: 0.7832\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.4130 - accuracy: 0.7905 - val_loss: 0.4194 - val_accuracy: 0.7835\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.4113 - accuracy: 0.7908 - val_loss: 0.4183 - val_accuracy: 0.7834\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.4107 - accuracy: 0.7899 - val_loss: 0.4162 - val_accuracy: 0.7832\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.4077 - accuracy: 0.7926 - val_loss: 0.4152 - val_accuracy: 0.7838\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.4068 - accuracy: 0.7920 - val_loss: 0.4146 - val_accuracy: 0.7862\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.4052 - accuracy: 0.7939 - val_loss: 0.4144 - val_accuracy: 0.7841\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.78      0.76     10216\n",
      "           1       0.82      0.79      0.81     13579\n",
      "\n",
      "    accuracy                           0.78     23795\n",
      "   macro avg       0.78      0.78      0.78     23795\n",
      "weighted avg       0.79      0.78      0.78     23795\n",
      "\n",
      "network= DS_128 DS_64 BN DO DS_32 DS_8\n",
      "optimizer= SGD \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 2s 17us/step - loss: 0.6796 - accuracy: 0.5691 - val_loss: 0.6759 - val_accuracy: 0.5707\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.6721 - accuracy: 0.5691 - val_loss: 0.6653 - val_accuracy: 0.5707\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.6564 - accuracy: 0.5781 - val_loss: 0.6413 - val_accuracy: 0.6088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.6209 - accuracy: 0.6697 - val_loss: 0.5925 - val_accuracy: 0.7225\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.5654 - accuracy: 0.7363 - val_loss: 0.5371 - val_accuracy: 0.7499\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.5180 - accuracy: 0.7578 - val_loss: 0.5017 - val_accuracy: 0.7603\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 2s 16us/step - loss: 0.4890 - accuracy: 0.7680 - val_loss: 0.4818 - val_accuracy: 0.7668\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.4734 - accuracy: 0.7728 - val_loss: 0.4696 - val_accuracy: 0.7715\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.4635 - accuracy: 0.7747 - val_loss: 0.4618 - val_accuracy: 0.7740\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.4574 - accuracy: 0.7767 - val_loss: 0.4564 - val_accuracy: 0.7746\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.4532 - accuracy: 0.7778 - val_loss: 0.4523 - val_accuracy: 0.7762\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 1s 14us/step - loss: 0.4505 - accuracy: 0.7779 - val_loss: 0.4495 - val_accuracy: 0.7769\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.4478 - accuracy: 0.7780 - val_loss: 0.4468 - val_accuracy: 0.7777\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.4461 - accuracy: 0.7795 - val_loss: 0.4450 - val_accuracy: 0.7772\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 1s 15us/step - loss: 0.4438 - accuracy: 0.7789 - val_loss: 0.4434 - val_accuracy: 0.7779\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.81      0.76     10216\n",
      "           1       0.84      0.76      0.80     13579\n",
      "\n",
      "    accuracy                           0.78     23795\n",
      "   macro avg       0.78      0.78      0.78     23795\n",
      "weighted avg       0.79      0.78      0.78     23795\n",
      "\n",
      "network= DS_256 DS_128 BN DO DS_64 DS_32 BN DO DS_16 DS_8\n",
      "optimizer= RMSprop \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 3s 26us/step - loss: 0.4914 - accuracy: 0.7651 - val_loss: 0.4514 - val_accuracy: 0.7756\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 2s 24us/step - loss: 0.4414 - accuracy: 0.7772 - val_loss: 0.4470 - val_accuracy: 0.7670\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 2s 21us/step - loss: 0.4278 - accuracy: 0.7825 - val_loss: 0.4325 - val_accuracy: 0.7824\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.4204 - accuracy: 0.7854 - val_loss: 0.4221 - val_accuracy: 0.7836\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.4144 - accuracy: 0.7881 - val_loss: 0.4174 - val_accuracy: 0.7852\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.4097 - accuracy: 0.7922 - val_loss: 0.4106 - val_accuracy: 0.7859\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.4069 - accuracy: 0.7927 - val_loss: 0.4132 - val_accuracy: 0.7869\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.4036 - accuracy: 0.7947 - val_loss: 0.4077 - val_accuracy: 0.7874\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.4023 - accuracy: 0.7960 - val_loss: 0.4094 - val_accuracy: 0.7873\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.3988 - accuracy: 0.7989 - val_loss: 0.4112 - val_accuracy: 0.7862\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.3973 - accuracy: 0.7984 - val_loss: 0.4094 - val_accuracy: 0.7896\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.3955 - accuracy: 0.8006 - val_loss: 0.4061 - val_accuracy: 0.7908\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.3930 - accuracy: 0.8021 - val_loss: 0.4132 - val_accuracy: 0.7880\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.3920 - accuracy: 0.8045 - val_loss: 0.4099 - val_accuracy: 0.7878\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.3897 - accuracy: 0.8056 - val_loss: 0.4125 - val_accuracy: 0.7876\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.83      0.77     10216\n",
      "           1       0.85      0.76      0.80     13579\n",
      "\n",
      "    accuracy                           0.79     23795\n",
      "   macro avg       0.79      0.79      0.79     23795\n",
      "weighted avg       0.80      0.79      0.79     23795\n",
      "\n",
      "network= DS_256 DS_128 BN DO DS_64 DS_32 BN DO DS_16 DS_8\n",
      "optimizer= Adam \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 3s 29us/step - loss: 0.4895 - accuracy: 0.7581 - val_loss: 0.4480 - val_accuracy: 0.7744\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4376 - accuracy: 0.7759 - val_loss: 0.4302 - val_accuracy: 0.7784\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4237 - accuracy: 0.7809 - val_loss: 0.4206 - val_accuracy: 0.7809\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 2s 24us/step - loss: 0.4164 - accuracy: 0.7842 - val_loss: 0.4226 - val_accuracy: 0.7796\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4103 - accuracy: 0.7895 - val_loss: 0.4130 - val_accuracy: 0.7852\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4059 - accuracy: 0.7922 - val_loss: 0.4121 - val_accuracy: 0.7850\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 2s 24us/step - loss: 0.4017 - accuracy: 0.7946 - val_loss: 0.4081 - val_accuracy: 0.7865\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3979 - accuracy: 0.7976 - val_loss: 0.4113 - val_accuracy: 0.7847\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3941 - accuracy: 0.7995 - val_loss: 0.4065 - val_accuracy: 0.7874\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3905 - accuracy: 0.8006 - val_loss: 0.4076 - val_accuracy: 0.7903\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3867 - accuracy: 0.8030 - val_loss: 0.4098 - val_accuracy: 0.7871\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3832 - accuracy: 0.8057 - val_loss: 0.4101 - val_accuracy: 0.7864\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3788 - accuracy: 0.8094 - val_loss: 0.4057 - val_accuracy: 0.7899\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3738 - accuracy: 0.8117 - val_loss: 0.4122 - val_accuracy: 0.7895\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3701 - accuracy: 0.8145 - val_loss: 0.4147 - val_accuracy: 0.7861\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.70      0.74     10216\n",
      "           1       0.79      0.85      0.82     13579\n",
      "\n",
      "    accuracy                           0.79     23795\n",
      "   macro avg       0.78      0.78      0.78     23795\n",
      "weighted avg       0.79      0.79      0.78     23795\n",
      "\n",
      "network= DS_256 DS_128 BN DO DS_64 DS_32 BN DO DS_16 DS_8\n",
      "optimizer= Adadelta \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 3s 30us/step - loss: 0.4905 - accuracy: 0.7600 - val_loss: 0.4514 - val_accuracy: 0.7741\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 2s 24us/step - loss: 0.4415 - accuracy: 0.7780 - val_loss: 0.4320 - val_accuracy: 0.7784\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 2s 24us/step - loss: 0.4278 - accuracy: 0.7788 - val_loss: 0.4227 - val_accuracy: 0.7811\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 2s 24us/step - loss: 0.4202 - accuracy: 0.7816 - val_loss: 0.4188 - val_accuracy: 0.7828\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95178/95178 [==============================] - 2s 24us/step - loss: 0.4152 - accuracy: 0.7858 - val_loss: 0.4191 - val_accuracy: 0.7836\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 2s 24us/step - loss: 0.4114 - accuracy: 0.7871 - val_loss: 0.4141 - val_accuracy: 0.7826\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 2s 24us/step - loss: 0.4084 - accuracy: 0.7905 - val_loss: 0.4175 - val_accuracy: 0.7843\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 2s 24us/step - loss: 0.4058 - accuracy: 0.7923 - val_loss: 0.4143 - val_accuracy: 0.7845\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 2s 24us/step - loss: 0.4033 - accuracy: 0.7933 - val_loss: 0.4128 - val_accuracy: 0.7869\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 2s 24us/step - loss: 0.4003 - accuracy: 0.7971 - val_loss: 0.4140 - val_accuracy: 0.7873\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 2s 25us/step - loss: 0.3975 - accuracy: 0.7980 - val_loss: 0.4111 - val_accuracy: 0.7879\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 2s 26us/step - loss: 0.3954 - accuracy: 0.7994 - val_loss: 0.4082 - val_accuracy: 0.7888\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 2s 24us/step - loss: 0.3932 - accuracy: 0.8019 - val_loss: 0.4059 - val_accuracy: 0.7914\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 2s 25us/step - loss: 0.3915 - accuracy: 0.8021 - val_loss: 0.4116 - val_accuracy: 0.7897\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 2s 25us/step - loss: 0.3890 - accuracy: 0.8039 - val_loss: 0.4065 - val_accuracy: 0.7905\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.75      0.75     10216\n",
      "           1       0.81      0.82      0.82     13579\n",
      "\n",
      "    accuracy                           0.79     23795\n",
      "   macro avg       0.79      0.79      0.79     23795\n",
      "weighted avg       0.79      0.79      0.79     23795\n",
      "\n",
      "network= DS_256 DS_128 BN DO DS_64 DS_32 BN DO DS_16 DS_8\n",
      "optimizer= Nadam \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 3s 32us/step - loss: 0.4729 - accuracy: 0.7665 - val_loss: 0.4346 - val_accuracy: 0.7742\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 2s 26us/step - loss: 0.4264 - accuracy: 0.7783 - val_loss: 0.4230 - val_accuracy: 0.7816\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 2s 26us/step - loss: 0.4158 - accuracy: 0.7845 - val_loss: 0.4235 - val_accuracy: 0.7826\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 2s 26us/step - loss: 0.4108 - accuracy: 0.7880 - val_loss: 0.4179 - val_accuracy: 0.7820\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 2s 26us/step - loss: 0.4045 - accuracy: 0.7915 - val_loss: 0.4133 - val_accuracy: 0.7812\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 2s 26us/step - loss: 0.4002 - accuracy: 0.7947 - val_loss: 0.4085 - val_accuracy: 0.7886\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 3s 27us/step - loss: 0.3953 - accuracy: 0.7980 - val_loss: 0.4060 - val_accuracy: 0.7876\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 2s 26us/step - loss: 0.3903 - accuracy: 0.8008 - val_loss: 0.4066 - val_accuracy: 0.7897\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 2s 26us/step - loss: 0.3844 - accuracy: 0.8055 - val_loss: 0.4121 - val_accuracy: 0.7864\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 2s 25us/step - loss: 0.3794 - accuracy: 0.8063 - val_loss: 0.4085 - val_accuracy: 0.7889\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 2s 25us/step - loss: 0.3719 - accuracy: 0.8119 - val_loss: 0.4107 - val_accuracy: 0.7866\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 2s 25us/step - loss: 0.3644 - accuracy: 0.8178 - val_loss: 0.4181 - val_accuracy: 0.7866\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 2s 25us/step - loss: 0.3563 - accuracy: 0.8229 - val_loss: 0.4184 - val_accuracy: 0.7817\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 2s 25us/step - loss: 0.3477 - accuracy: 0.8284 - val_loss: 0.4227 - val_accuracy: 0.7767\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 2s 25us/step - loss: 0.3388 - accuracy: 0.8351 - val_loss: 0.4379 - val_accuracy: 0.7814\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.85      0.77     10216\n",
      "           1       0.86      0.73      0.79     13579\n",
      "\n",
      "    accuracy                           0.78     23795\n",
      "   macro avg       0.78      0.79      0.78     23795\n",
      "weighted avg       0.80      0.78      0.78     23795\n",
      "\n",
      "network= DS_256 DS_128 BN DO DS_64 DS_32 BN DO DS_16 DS_8\n",
      "optimizer= Adamax \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 3s 29us/step - loss: 0.4815 - accuracy: 0.7670 - val_loss: 0.4513 - val_accuracy: 0.7770\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 2s 25us/step - loss: 0.4406 - accuracy: 0.7788 - val_loss: 0.4347 - val_accuracy: 0.7797\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4277 - accuracy: 0.7804 - val_loss: 0.4225 - val_accuracy: 0.7837\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 2s 25us/step - loss: 0.4209 - accuracy: 0.7826 - val_loss: 0.4190 - val_accuracy: 0.7832\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 2s 25us/step - loss: 0.4147 - accuracy: 0.7864 - val_loss: 0.4175 - val_accuracy: 0.7840\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 2s 24us/step - loss: 0.4103 - accuracy: 0.7897 - val_loss: 0.4128 - val_accuracy: 0.7837\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 2s 24us/step - loss: 0.4055 - accuracy: 0.7921 - val_loss: 0.4112 - val_accuracy: 0.7869\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 2s 24us/step - loss: 0.4019 - accuracy: 0.7935 - val_loss: 0.4094 - val_accuracy: 0.7860\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3982 - accuracy: 0.7967 - val_loss: 0.4051 - val_accuracy: 0.7874\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3956 - accuracy: 0.7991 - val_loss: 0.4095 - val_accuracy: 0.7868\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 2s 24us/step - loss: 0.3923 - accuracy: 0.7999 - val_loss: 0.4066 - val_accuracy: 0.7875\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 2s 24us/step - loss: 0.3903 - accuracy: 0.8021 - val_loss: 0.4074 - val_accuracy: 0.7883\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 2s 24us/step - loss: 0.3881 - accuracy: 0.8015 - val_loss: 0.4140 - val_accuracy: 0.7852\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 2s 24us/step - loss: 0.3857 - accuracy: 0.8043 - val_loss: 0.4073 - val_accuracy: 0.7877\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3825 - accuracy: 0.8070 - val_loss: 0.4067 - val_accuracy: 0.7906\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.75      0.76     10216\n",
      "           1       0.82      0.82      0.82     13579\n",
      "\n",
      "    accuracy                           0.79     23795\n",
      "   macro avg       0.79      0.79      0.79     23795\n",
      "weighted avg       0.79      0.79      0.79     23795\n",
      "\n",
      "network= DS_256 DS_128 BN DO DS_64 DS_32 BN DO DS_16 DS_8\n",
      "optimizer= Adagrad \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 3s 28us/step - loss: 0.4884 - accuracy: 0.7697 - val_loss: 0.4597 - val_accuracy: 0.7753\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4493 - accuracy: 0.7796 - val_loss: 0.4442 - val_accuracy: 0.7798\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4372 - accuracy: 0.7817 - val_loss: 0.4357 - val_accuracy: 0.7818\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4298 - accuracy: 0.7830 - val_loss: 0.4289 - val_accuracy: 0.7825\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4246 - accuracy: 0.7840 - val_loss: 0.4239 - val_accuracy: 0.7832\n",
      "Epoch 6/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4214 - accuracy: 0.7847 - val_loss: 0.4224 - val_accuracy: 0.7838\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4178 - accuracy: 0.7875 - val_loss: 0.4201 - val_accuracy: 0.7837\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4154 - accuracy: 0.7878 - val_loss: 0.4177 - val_accuracy: 0.7855\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4134 - accuracy: 0.7894 - val_loss: 0.4173 - val_accuracy: 0.7854\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.4118 - accuracy: 0.7897 - val_loss: 0.4168 - val_accuracy: 0.7859\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4104 - accuracy: 0.7917 - val_loss: 0.4165 - val_accuracy: 0.7873\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4084 - accuracy: 0.7911 - val_loss: 0.4144 - val_accuracy: 0.7877\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4067 - accuracy: 0.7934 - val_loss: 0.4143 - val_accuracy: 0.7865\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.4051 - accuracy: 0.7955 - val_loss: 0.4137 - val_accuracy: 0.7864\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4039 - accuracy: 0.7954 - val_loss: 0.4123 - val_accuracy: 0.7861\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.77      0.75     10216\n",
      "           1       0.82      0.80      0.81     13579\n",
      "\n",
      "    accuracy                           0.79     23795\n",
      "   macro avg       0.78      0.78      0.78     23795\n",
      "weighted avg       0.79      0.79      0.79     23795\n",
      "\n",
      "network= DS_256 DS_128 BN DO DS_64 DS_32 BN DO DS_16 DS_8\n",
      "optimizer= SGD \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 3s 27us/step - loss: 0.6777 - accuracy: 0.5647 - val_loss: 0.6537 - val_accuracy: 0.5937\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.6304 - accuracy: 0.6691 - val_loss: 0.5938 - val_accuracy: 0.7225\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.5666 - accuracy: 0.7413 - val_loss: 0.5281 - val_accuracy: 0.7600\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.5113 - accuracy: 0.7649 - val_loss: 0.4901 - val_accuracy: 0.7690\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.4847 - accuracy: 0.7695 - val_loss: 0.4728 - val_accuracy: 0.7748\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.4714 - accuracy: 0.7724 - val_loss: 0.4628 - val_accuracy: 0.7760\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.4647 - accuracy: 0.7738 - val_loss: 0.4573 - val_accuracy: 0.7779\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.4597 - accuracy: 0.7756 - val_loss: 0.4537 - val_accuracy: 0.7780\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.4564 - accuracy: 0.7763 - val_loss: 0.4506 - val_accuracy: 0.7768\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4529 - accuracy: 0.7760 - val_loss: 0.4482 - val_accuracy: 0.7772\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4509 - accuracy: 0.7769 - val_loss: 0.4458 - val_accuracy: 0.7778\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 2s 24us/step - loss: 0.4483 - accuracy: 0.7776 - val_loss: 0.4447 - val_accuracy: 0.7778\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4483 - accuracy: 0.7782 - val_loss: 0.4429 - val_accuracy: 0.7777\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.4467 - accuracy: 0.7789 - val_loss: 0.4422 - val_accuracy: 0.7765\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 2s 21us/step - loss: 0.4449 - accuracy: 0.7783 - val_loss: 0.4405 - val_accuracy: 0.7791\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.82      0.76     10216\n",
      "           1       0.85      0.75      0.79     13579\n",
      "\n",
      "    accuracy                           0.78     23795\n",
      "   macro avg       0.78      0.78      0.78     23795\n",
      "weighted avg       0.79      0.78      0.78     23795\n",
      "\n",
      "network= DR_256 DR_512 BN DR_512 DR_256 BN DO DR_512 BN DO\n",
      "optimizer= RMSprop \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 7s 71us/step - loss: 0.4788 - accuracy: 0.7567 - val_loss: 0.4350 - val_accuracy: 0.7733\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 6s 65us/step - loss: 0.4247 - accuracy: 0.7802 - val_loss: 0.4421 - val_accuracy: 0.7692\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 6s 65us/step - loss: 0.4102 - accuracy: 0.7905 - val_loss: 0.4205 - val_accuracy: 0.7820\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 6s 67us/step - loss: 0.4007 - accuracy: 0.7968 - val_loss: 0.4179 - val_accuracy: 0.7866\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 6s 65us/step - loss: 0.3942 - accuracy: 0.8013 - val_loss: 0.4100 - val_accuracy: 0.7882\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 6s 65us/step - loss: 0.3862 - accuracy: 0.8070 - val_loss: 0.4120 - val_accuracy: 0.7897\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 6s 65us/step - loss: 0.3797 - accuracy: 0.8117 - val_loss: 0.4220 - val_accuracy: 0.7873\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 6s 65us/step - loss: 0.3718 - accuracy: 0.8174 - val_loss: 0.4198 - val_accuracy: 0.7900\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 6s 65us/step - loss: 0.3638 - accuracy: 0.8232 - val_loss: 0.4414 - val_accuracy: 0.7790\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 6s 65us/step - loss: 0.3544 - accuracy: 0.8299 - val_loss: 0.4249 - val_accuracy: 0.7832\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 6s 65us/step - loss: 0.3460 - accuracy: 0.8357 - val_loss: 0.4329 - val_accuracy: 0.7791\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 6s 65us/step - loss: 0.3365 - accuracy: 0.8414 - val_loss: 0.4381 - val_accuracy: 0.7823\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 6s 65us/step - loss: 0.3267 - accuracy: 0.8494 - val_loss: 0.4659 - val_accuracy: 0.7764\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 6s 65us/step - loss: 0.3166 - accuracy: 0.8546 - val_loss: 0.4442 - val_accuracy: 0.7774\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 6s 65us/step - loss: 0.3063 - accuracy: 0.8604 - val_loss: 0.4694 - val_accuracy: 0.7767\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.72      0.74     10216\n",
      "           1       0.80      0.82      0.81     13579\n",
      "\n",
      "    accuracy                           0.78     23795\n",
      "   macro avg       0.77      0.77      0.77     23795\n",
      "weighted avg       0.78      0.78      0.78     23795\n",
      "\n",
      "network= DR_256 DR_512 BN DR_512 DR_256 BN DO DR_512 BN DO\n",
      "optimizer= Adam \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 7s 76us/step - loss: 0.4749 - accuracy: 0.7571 - val_loss: 0.4301 - val_accuracy: 0.7759\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 7s 73us/step - loss: 0.4201 - accuracy: 0.7820 - val_loss: 0.4251 - val_accuracy: 0.7794\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 7s 72us/step - loss: 0.4068 - accuracy: 0.7899 - val_loss: 0.4098 - val_accuracy: 0.7861\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 7s 74us/step - loss: 0.3977 - accuracy: 0.7952 - val_loss: 0.4127 - val_accuracy: 0.7881\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 7s 71us/step - loss: 0.3910 - accuracy: 0.7998 - val_loss: 0.4114 - val_accuracy: 0.7883\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 7s 73us/step - loss: 0.3833 - accuracy: 0.8053 - val_loss: 0.4157 - val_accuracy: 0.7882\n",
      "Epoch 7/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95178/95178 [==============================] - 7s 74us/step - loss: 0.3784 - accuracy: 0.8074 - val_loss: 0.4129 - val_accuracy: 0.7858\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 7s 72us/step - loss: 0.3695 - accuracy: 0.8154 - val_loss: 0.4147 - val_accuracy: 0.7879\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 7s 71us/step - loss: 0.3617 - accuracy: 0.8188 - val_loss: 0.4229 - val_accuracy: 0.7854\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 7s 72us/step - loss: 0.3519 - accuracy: 0.8263 - val_loss: 0.4348 - val_accuracy: 0.7832\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 7s 73us/step - loss: 0.3409 - accuracy: 0.8329 - val_loss: 0.4219 - val_accuracy: 0.7829\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 7s 72us/step - loss: 0.3289 - accuracy: 0.8403 - val_loss: 0.4397 - val_accuracy: 0.7795\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 7s 72us/step - loss: 0.3151 - accuracy: 0.8496 - val_loss: 0.4579 - val_accuracy: 0.7781\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 7s 72us/step - loss: 0.3013 - accuracy: 0.8573 - val_loss: 0.4844 - val_accuracy: 0.7759\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 7s 72us/step - loss: 0.2868 - accuracy: 0.8652 - val_loss: 0.4935 - val_accuracy: 0.7739\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.75      0.74     10216\n",
      "           1       0.81      0.79      0.80     13579\n",
      "\n",
      "    accuracy                           0.77     23795\n",
      "   macro avg       0.77      0.77      0.77     23795\n",
      "weighted avg       0.77      0.77      0.77     23795\n",
      "\n",
      "network= DR_256 DR_512 BN DR_512 DR_256 BN DO DR_512 BN DO\n",
      "optimizer= Adadelta \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 8s 83us/step - loss: 0.4746 - accuracy: 0.7589 - val_loss: 0.4365 - val_accuracy: 0.7771\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 7s 77us/step - loss: 0.4192 - accuracy: 0.7813 - val_loss: 0.4299 - val_accuracy: 0.7818\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 7s 76us/step - loss: 0.4045 - accuracy: 0.7923 - val_loss: 0.4110 - val_accuracy: 0.7852\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 7s 77us/step - loss: 0.3940 - accuracy: 0.8008 - val_loss: 0.4079 - val_accuracy: 0.7864\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 7s 77us/step - loss: 0.3850 - accuracy: 0.8052 - val_loss: 0.4203 - val_accuracy: 0.7888\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 7s 77us/step - loss: 0.3768 - accuracy: 0.8112 - val_loss: 0.4198 - val_accuracy: 0.7877\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 7s 76us/step - loss: 0.3682 - accuracy: 0.8175 - val_loss: 0.4207 - val_accuracy: 0.7883\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 7s 77us/step - loss: 0.3582 - accuracy: 0.8233 - val_loss: 0.4253 - val_accuracy: 0.7826\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 7s 77us/step - loss: 0.3465 - accuracy: 0.8315 - val_loss: 0.4413 - val_accuracy: 0.7842\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 7s 77us/step - loss: 0.3346 - accuracy: 0.8392 - val_loss: 0.4411 - val_accuracy: 0.7828\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 7s 77us/step - loss: 0.3203 - accuracy: 0.8489 - val_loss: 0.4600 - val_accuracy: 0.7816\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 7s 77us/step - loss: 0.3054 - accuracy: 0.8566 - val_loss: 0.4674 - val_accuracy: 0.7745\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 7s 76us/step - loss: 0.2913 - accuracy: 0.8656 - val_loss: 0.4884 - val_accuracy: 0.7761\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 7s 77us/step - loss: 0.2739 - accuracy: 0.8756 - val_loss: 0.5131 - val_accuracy: 0.7721\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 7s 77us/step - loss: 0.2588 - accuracy: 0.8834 - val_loss: 0.5212 - val_accuracy: 0.7721\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.74      0.73     10216\n",
      "           1       0.80      0.80      0.80     13579\n",
      "\n",
      "    accuracy                           0.77     23795\n",
      "   macro avg       0.77      0.77      0.77     23795\n",
      "weighted avg       0.77      0.77      0.77     23795\n",
      "\n",
      "network= DR_256 DR_512 BN DR_512 DR_256 BN DO DR_512 BN DO\n",
      "optimizer= Nadam \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 8s 82us/step - loss: 0.4603 - accuracy: 0.7622 - val_loss: 0.4316 - val_accuracy: 0.7721\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 7s 77us/step - loss: 0.4167 - accuracy: 0.7822 - val_loss: 0.4150 - val_accuracy: 0.7861\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 7s 78us/step - loss: 0.4061 - accuracy: 0.7907 - val_loss: 0.4098 - val_accuracy: 0.7862\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 7s 78us/step - loss: 0.3981 - accuracy: 0.7964 - val_loss: 0.4083 - val_accuracy: 0.7882\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 7s 78us/step - loss: 0.3922 - accuracy: 0.8001 - val_loss: 0.4159 - val_accuracy: 0.7867\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 7s 74us/step - loss: 0.3867 - accuracy: 0.8039 - val_loss: 0.4095 - val_accuracy: 0.7876\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 7s 78us/step - loss: 0.3793 - accuracy: 0.8081 - val_loss: 0.4125 - val_accuracy: 0.7842\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 7s 78us/step - loss: 0.3736 - accuracy: 0.8119 - val_loss: 0.4156 - val_accuracy: 0.7882\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 7s 78us/step - loss: 0.3660 - accuracy: 0.8150 - val_loss: 0.4184 - val_accuracy: 0.7827\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 7s 78us/step - loss: 0.3574 - accuracy: 0.8210 - val_loss: 0.4258 - val_accuracy: 0.7871\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 7s 78us/step - loss: 0.3485 - accuracy: 0.8279 - val_loss: 0.4292 - val_accuracy: 0.7881\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 7s 77us/step - loss: 0.3385 - accuracy: 0.8337 - val_loss: 0.4318 - val_accuracy: 0.7803\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 7s 74us/step - loss: 0.3284 - accuracy: 0.8397 - val_loss: 0.4381 - val_accuracy: 0.7805\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 7s 76us/step - loss: 0.3172 - accuracy: 0.8476 - val_loss: 0.4339 - val_accuracy: 0.7823\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 7s 79us/step - loss: 0.3038 - accuracy: 0.8541 - val_loss: 0.4738 - val_accuracy: 0.7708\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.76      0.74     10216\n",
      "           1       0.81      0.78      0.80     13579\n",
      "\n",
      "    accuracy                           0.77     23795\n",
      "   macro avg       0.77      0.77      0.77     23795\n",
      "weighted avg       0.77      0.77      0.77     23795\n",
      "\n",
      "network= DR_256 DR_512 BN DR_512 DR_256 BN DO DR_512 BN DO\n",
      "optimizer= Adamax \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 8s 80us/step - loss: 0.4796 - accuracy: 0.7534 - val_loss: 0.4314 - val_accuracy: 0.7762\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 7s 73us/step - loss: 0.4220 - accuracy: 0.7811 - val_loss: 0.4295 - val_accuracy: 0.7764\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 7s 72us/step - loss: 0.4095 - accuracy: 0.7894 - val_loss: 0.4183 - val_accuracy: 0.7823\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 7s 73us/step - loss: 0.3977 - accuracy: 0.7967 - val_loss: 0.4098 - val_accuracy: 0.7867\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 7s 72us/step - loss: 0.3869 - accuracy: 0.8046 - val_loss: 0.4100 - val_accuracy: 0.7884\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 7s 72us/step - loss: 0.3757 - accuracy: 0.8111 - val_loss: 0.4187 - val_accuracy: 0.7863\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 7s 73us/step - loss: 0.3629 - accuracy: 0.8185 - val_loss: 0.4218 - val_accuracy: 0.7858\n",
      "Epoch 8/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95178/95178 [==============================] - 7s 73us/step - loss: 0.3491 - accuracy: 0.8269 - val_loss: 0.4352 - val_accuracy: 0.7861\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 7s 73us/step - loss: 0.3331 - accuracy: 0.8370 - val_loss: 0.4557 - val_accuracy: 0.7797\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 7s 73us/step - loss: 0.3176 - accuracy: 0.8458 - val_loss: 0.4559 - val_accuracy: 0.7761\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 7s 71us/step - loss: 0.2965 - accuracy: 0.8569 - val_loss: 0.4869 - val_accuracy: 0.7720\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 7s 73us/step - loss: 0.2765 - accuracy: 0.8686 - val_loss: 0.4950 - val_accuracy: 0.7681\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 7s 73us/step - loss: 0.2561 - accuracy: 0.8805 - val_loss: 0.5310 - val_accuracy: 0.7636\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 7s 73us/step - loss: 0.2314 - accuracy: 0.8931 - val_loss: 0.6002 - val_accuracy: 0.7647\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 7s 72us/step - loss: 0.2145 - accuracy: 0.9019 - val_loss: 0.6444 - val_accuracy: 0.7611\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.72      0.72     10216\n",
      "           1       0.79      0.79      0.79     13579\n",
      "\n",
      "    accuracy                           0.76     23795\n",
      "   macro avg       0.76      0.76      0.76     23795\n",
      "weighted avg       0.76      0.76      0.76     23795\n",
      "\n",
      "network= DR_256 DR_512 BN DR_512 DR_256 BN DO DR_512 BN DO\n",
      "optimizer= Adagrad \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 7s 70us/step - loss: 0.4657 - accuracy: 0.7603 - val_loss: 0.4246 - val_accuracy: 0.7763\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 6s 64us/step - loss: 0.4151 - accuracy: 0.7845 - val_loss: 0.4164 - val_accuracy: 0.7823\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 6s 65us/step - loss: 0.4001 - accuracy: 0.7947 - val_loss: 0.4126 - val_accuracy: 0.7853\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 6s 65us/step - loss: 0.3877 - accuracy: 0.8012 - val_loss: 0.4145 - val_accuracy: 0.7840\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 6s 65us/step - loss: 0.3769 - accuracy: 0.8098 - val_loss: 0.4150 - val_accuracy: 0.7829\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 6s 65us/step - loss: 0.3655 - accuracy: 0.8157 - val_loss: 0.4234 - val_accuracy: 0.7813\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 6s 65us/step - loss: 0.3543 - accuracy: 0.8217 - val_loss: 0.4306 - val_accuracy: 0.7815\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 6s 65us/step - loss: 0.3404 - accuracy: 0.8312 - val_loss: 0.4410 - val_accuracy: 0.7768\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 6s 65us/step - loss: 0.3277 - accuracy: 0.8378 - val_loss: 0.4520 - val_accuracy: 0.7747\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 6s 65us/step - loss: 0.3133 - accuracy: 0.8467 - val_loss: 0.4742 - val_accuracy: 0.7717\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 6s 65us/step - loss: 0.2969 - accuracy: 0.8563 - val_loss: 0.5025 - val_accuracy: 0.7689\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 6s 65us/step - loss: 0.2821 - accuracy: 0.8653 - val_loss: 0.5161 - val_accuracy: 0.7681\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 6s 65us/step - loss: 0.2649 - accuracy: 0.8755 - val_loss: 0.5465 - val_accuracy: 0.7680\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 6s 65us/step - loss: 0.2476 - accuracy: 0.8853 - val_loss: 0.5776 - val_accuracy: 0.7645\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 6s 65us/step - loss: 0.2316 - accuracy: 0.8936 - val_loss: 0.6097 - val_accuracy: 0.7615\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.74      0.73     10216\n",
      "           1       0.80      0.78      0.79     13579\n",
      "\n",
      "    accuracy                           0.76     23795\n",
      "   macro avg       0.76      0.76      0.76     23795\n",
      "weighted avg       0.76      0.76      0.76     23795\n",
      "\n",
      "network= DR_256 DR_512 BN DR_512 DR_256 BN DO DR_512 BN DO\n",
      "optimizer= SGD \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 7s 70us/step - loss: 0.5373 - accuracy: 0.7293 - val_loss: 0.4606 - val_accuracy: 0.7603\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 6s 63us/step - loss: 0.4515 - accuracy: 0.7689 - val_loss: 0.4444 - val_accuracy: 0.7685\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 6s 64us/step - loss: 0.4306 - accuracy: 0.7821 - val_loss: 0.4381 - val_accuracy: 0.7705\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 6s 64us/step - loss: 0.4193 - accuracy: 0.7875 - val_loss: 0.4347 - val_accuracy: 0.7748\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 6s 64us/step - loss: 0.4104 - accuracy: 0.7923 - val_loss: 0.4309 - val_accuracy: 0.7758\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 6s 64us/step - loss: 0.4035 - accuracy: 0.7977 - val_loss: 0.4323 - val_accuracy: 0.7746\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 6s 65us/step - loss: 0.3951 - accuracy: 0.8007 - val_loss: 0.4312 - val_accuracy: 0.7746\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 6s 64us/step - loss: 0.3893 - accuracy: 0.8051 - val_loss: 0.4305 - val_accuracy: 0.7736\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 6s 65us/step - loss: 0.3820 - accuracy: 0.8103 - val_loss: 0.4304 - val_accuracy: 0.7751\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 6s 64us/step - loss: 0.3757 - accuracy: 0.8134 - val_loss: 0.4307 - val_accuracy: 0.7786\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 6s 65us/step - loss: 0.3696 - accuracy: 0.8178 - val_loss: 0.4322 - val_accuracy: 0.7760\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 6s 64us/step - loss: 0.3632 - accuracy: 0.8215 - val_loss: 0.4351 - val_accuracy: 0.7756\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 6s 64us/step - loss: 0.3564 - accuracy: 0.8260 - val_loss: 0.4380 - val_accuracy: 0.7749\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 6s 64us/step - loss: 0.3493 - accuracy: 0.8303 - val_loss: 0.4432 - val_accuracy: 0.7725\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 6s 64us/step - loss: 0.3430 - accuracy: 0.8341 - val_loss: 0.4475 - val_accuracy: 0.7742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.76      0.74     10216\n",
      "           1       0.81      0.78      0.80     13579\n",
      "\n",
      "    accuracy                           0.77     23795\n",
      "   macro avg       0.77      0.77      0.77     23795\n",
      "weighted avg       0.78      0.77      0.77     23795\n",
      "\n",
      "network= DS_256 DS_512 BN DS_512 DS_256 BN DO DS_512 BN DO\n",
      "optimizer= RMSprop \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 7s 74us/step - loss: 0.4854 - accuracy: 0.7526 - val_loss: 0.4741 - val_accuracy: 0.7278\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 6s 67us/step - loss: 0.4330 - accuracy: 0.7735 - val_loss: 0.4241 - val_accuracy: 0.7789\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 6s 68us/step - loss: 0.4252 - accuracy: 0.7801 - val_loss: 0.4401 - val_accuracy: 0.7711\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 6s 68us/step - loss: 0.4198 - accuracy: 0.7821 - val_loss: 0.4557 - val_accuracy: 0.7823\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 6s 68us/step - loss: 0.4148 - accuracy: 0.7857 - val_loss: 0.4247 - val_accuracy: 0.7791\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 6s 68us/step - loss: 0.4118 - accuracy: 0.7891 - val_loss: 0.4236 - val_accuracy: 0.7810\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 7s 73us/step - loss: 0.4086 - accuracy: 0.7921 - val_loss: 0.4191 - val_accuracy: 0.7776\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 6s 67us/step - loss: 0.4058 - accuracy: 0.7922 - val_loss: 0.4181 - val_accuracy: 0.7795\n",
      "Epoch 9/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95178/95178 [==============================] - 6s 67us/step - loss: 0.4042 - accuracy: 0.7940 - val_loss: 0.4110 - val_accuracy: 0.7868\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 6s 67us/step - loss: 0.4014 - accuracy: 0.7968 - val_loss: 0.4220 - val_accuracy: 0.7870\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 6s 67us/step - loss: 0.4003 - accuracy: 0.7983 - val_loss: 0.4124 - val_accuracy: 0.7884\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 6s 66us/step - loss: 0.3980 - accuracy: 0.7988 - val_loss: 0.4188 - val_accuracy: 0.7876\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 7s 69us/step - loss: 0.3959 - accuracy: 0.8002 - val_loss: 0.4131 - val_accuracy: 0.7856\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 6s 68us/step - loss: 0.3929 - accuracy: 0.8031 - val_loss: 0.4166 - val_accuracy: 0.7882\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 7s 69us/step - loss: 0.3910 - accuracy: 0.8030 - val_loss: 0.4173 - val_accuracy: 0.7880\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.73      0.75     10216\n",
      "           1       0.80      0.83      0.82     13579\n",
      "\n",
      "    accuracy                           0.79     23795\n",
      "   macro avg       0.78      0.78      0.78     23795\n",
      "weighted avg       0.79      0.79      0.79     23795\n",
      "\n",
      "network= DS_256 DS_512 BN DS_512 DS_256 BN DO DS_512 BN DO\n",
      "optimizer= Adam \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 8s 81us/step - loss: 0.4775 - accuracy: 0.7569 - val_loss: 0.4404 - val_accuracy: 0.7715\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 7s 72us/step - loss: 0.4336 - accuracy: 0.7729 - val_loss: 0.4261 - val_accuracy: 0.7752\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 7s 75us/step - loss: 0.4221 - accuracy: 0.7797 - val_loss: 0.4211 - val_accuracy: 0.7777\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 7s 76us/step - loss: 0.4164 - accuracy: 0.7827 - val_loss: 0.4255 - val_accuracy: 0.7779\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 7s 76us/step - loss: 0.4131 - accuracy: 0.7842 - val_loss: 0.4154 - val_accuracy: 0.7770\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 7s 73us/step - loss: 0.4074 - accuracy: 0.7897 - val_loss: 0.4149 - val_accuracy: 0.7852\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 7s 74us/step - loss: 0.4043 - accuracy: 0.7913 - val_loss: 0.4056 - val_accuracy: 0.7865\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 7s 75us/step - loss: 0.4020 - accuracy: 0.7919 - val_loss: 0.4095 - val_accuracy: 0.7823\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 7s 73us/step - loss: 0.3984 - accuracy: 0.7935 - val_loss: 0.4146 - val_accuracy: 0.7849\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 7s 73us/step - loss: 0.3951 - accuracy: 0.7967 - val_loss: 0.4139 - val_accuracy: 0.7882\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 7s 73us/step - loss: 0.3903 - accuracy: 0.7992 - val_loss: 0.4080 - val_accuracy: 0.7873\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 7s 73us/step - loss: 0.3880 - accuracy: 0.8012 - val_loss: 0.4107 - val_accuracy: 0.7820\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 7s 74us/step - loss: 0.3837 - accuracy: 0.8029 - val_loss: 0.4144 - val_accuracy: 0.7876\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 7s 73us/step - loss: 0.3784 - accuracy: 0.8066 - val_loss: 0.4286 - val_accuracy: 0.7879\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 7s 74us/step - loss: 0.3736 - accuracy: 0.8096 - val_loss: 0.4117 - val_accuracy: 0.7858\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.74      0.75     10216\n",
      "           1       0.81      0.82      0.81     13579\n",
      "\n",
      "    accuracy                           0.79     23795\n",
      "   macro avg       0.78      0.78      0.78     23795\n",
      "weighted avg       0.79      0.79      0.79     23795\n",
      "\n",
      "network= DS_256 DS_512 BN DS_512 DS_256 BN DO DS_512 BN DO\n",
      "optimizer= Adadelta \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 8s 87us/step - loss: 0.4821 - accuracy: 0.7541 - val_loss: 0.4494 - val_accuracy: 0.7666\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 7s 76us/step - loss: 0.4378 - accuracy: 0.7725 - val_loss: 0.4668 - val_accuracy: 0.7420\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 7s 79us/step - loss: 0.4256 - accuracy: 0.7776 - val_loss: 0.4384 - val_accuracy: 0.7595\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 7s 78us/step - loss: 0.4181 - accuracy: 0.7831 - val_loss: 0.4175 - val_accuracy: 0.7826\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 8s 79us/step - loss: 0.4125 - accuracy: 0.7854 - val_loss: 0.4178 - val_accuracy: 0.7839\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 7s 77us/step - loss: 0.4108 - accuracy: 0.7884 - val_loss: 0.4114 - val_accuracy: 0.7878\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 7s 78us/step - loss: 0.4072 - accuracy: 0.7894 - val_loss: 0.4202 - val_accuracy: 0.7814\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 8s 80us/step - loss: 0.4041 - accuracy: 0.7928 - val_loss: 0.4206 - val_accuracy: 0.7871\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 8s 81us/step - loss: 0.4023 - accuracy: 0.7923 - val_loss: 0.4048 - val_accuracy: 0.7888\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 7s 79us/step - loss: 0.4000 - accuracy: 0.7947 - val_loss: 0.4083 - val_accuracy: 0.7861\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 7s 78us/step - loss: 0.3976 - accuracy: 0.7960 - val_loss: 0.4092 - val_accuracy: 0.7866\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 7s 78us/step - loss: 0.3949 - accuracy: 0.7966 - val_loss: 0.4112 - val_accuracy: 0.7837\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 7s 78us/step - loss: 0.3927 - accuracy: 0.7992 - val_loss: 0.4215 - val_accuracy: 0.7819\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 8s 80us/step - loss: 0.3913 - accuracy: 0.8000 - val_loss: 0.4038 - val_accuracy: 0.7892\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 7s 77us/step - loss: 0.3873 - accuracy: 0.8032 - val_loss: 0.4107 - val_accuracy: 0.7876\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.83      0.77     10216\n",
      "           1       0.85      0.76      0.80     13579\n",
      "\n",
      "    accuracy                           0.79     23795\n",
      "   macro avg       0.79      0.79      0.79     23795\n",
      "weighted avg       0.80      0.79      0.79     23795\n",
      "\n",
      "network= DS_256 DS_512 BN DS_512 DS_256 BN DO DS_512 BN DO\n",
      "optimizer= Nadam \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 8s 86us/step - loss: 0.4723 - accuracy: 0.7576 - val_loss: 0.4319 - val_accuracy: 0.7712\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 8s 79us/step - loss: 0.4280 - accuracy: 0.7746 - val_loss: 0.4300 - val_accuracy: 0.7735\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 7s 78us/step - loss: 0.4208 - accuracy: 0.7795 - val_loss: 0.4501 - val_accuracy: 0.7721\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 7s 78us/step - loss: 0.4170 - accuracy: 0.7815 - val_loss: 0.4332 - val_accuracy: 0.7832\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 8s 80us/step - loss: 0.4125 - accuracy: 0.7860 - val_loss: 0.4146 - val_accuracy: 0.7835\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 8s 80us/step - loss: 0.4073 - accuracy: 0.7891 - val_loss: 0.4147 - val_accuracy: 0.7843\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 8s 80us/step - loss: 0.4035 - accuracy: 0.7911 - val_loss: 0.4102 - val_accuracy: 0.7863\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 8s 80us/step - loss: 0.3980 - accuracy: 0.7947 - val_loss: 0.4139 - val_accuracy: 0.7853\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 8s 79us/step - loss: 0.3939 - accuracy: 0.7995 - val_loss: 0.4177 - val_accuracy: 0.7874\n",
      "Epoch 10/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95178/95178 [==============================] - 8s 79us/step - loss: 0.3901 - accuracy: 0.8004 - val_loss: 0.4297 - val_accuracy: 0.7879\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 8s 79us/step - loss: 0.3848 - accuracy: 0.8049 - val_loss: 0.4124 - val_accuracy: 0.7825\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 8s 80us/step - loss: 0.3771 - accuracy: 0.8083 - val_loss: 0.4116 - val_accuracy: 0.7887\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 7s 79us/step - loss: 0.3711 - accuracy: 0.8128 - val_loss: 0.4150 - val_accuracy: 0.7855\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 8s 79us/step - loss: 0.3635 - accuracy: 0.8180 - val_loss: 0.4190 - val_accuracy: 0.7819\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 7s 78us/step - loss: 0.3557 - accuracy: 0.8217 - val_loss: 0.4339 - val_accuracy: 0.7856\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.73      0.74     10216\n",
      "           1       0.80      0.83      0.82     13579\n",
      "\n",
      "    accuracy                           0.79     23795\n",
      "   macro avg       0.78      0.78      0.78     23795\n",
      "weighted avg       0.78      0.79      0.78     23795\n",
      "\n",
      "network= DS_256 DS_512 BN DS_512 DS_256 BN DO DS_512 BN DO\n",
      "optimizer= Adamax \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 8s 85us/step - loss: 0.4777 - accuracy: 0.7566 - val_loss: 0.4426 - val_accuracy: 0.7663\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 7s 74us/step - loss: 0.4380 - accuracy: 0.7710 - val_loss: 0.4393 - val_accuracy: 0.7630\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 7s 76us/step - loss: 0.4280 - accuracy: 0.7748 - val_loss: 0.4258 - val_accuracy: 0.7814\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 7s 75us/step - loss: 0.4198 - accuracy: 0.7801 - val_loss: 0.4259 - val_accuracy: 0.7712\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 7s 73us/step - loss: 0.4149 - accuracy: 0.7830 - val_loss: 0.4219 - val_accuracy: 0.7709\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 7s 77us/step - loss: 0.4103 - accuracy: 0.7866 - val_loss: 0.4108 - val_accuracy: 0.7855\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 7s 76us/step - loss: 0.4069 - accuracy: 0.7873 - val_loss: 0.4143 - val_accuracy: 0.7833\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 7s 77us/step - loss: 0.4019 - accuracy: 0.7907 - val_loss: 0.4035 - val_accuracy: 0.7901\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 7s 79us/step - loss: 0.3987 - accuracy: 0.7942 - val_loss: 0.4040 - val_accuracy: 0.7870\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 7s 77us/step - loss: 0.3954 - accuracy: 0.7956 - val_loss: 0.4070 - val_accuracy: 0.7863\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 7s 76us/step - loss: 0.3921 - accuracy: 0.7984 - val_loss: 0.4027 - val_accuracy: 0.7887\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 7s 77us/step - loss: 0.3887 - accuracy: 0.7989 - val_loss: 0.4067 - val_accuracy: 0.7868\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 7s 79us/step - loss: 0.3851 - accuracy: 0.8030 - val_loss: 0.4159 - val_accuracy: 0.7857\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 8s 81us/step - loss: 0.3812 - accuracy: 0.8041 - val_loss: 0.4083 - val_accuracy: 0.7838\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 8s 82us/step - loss: 0.3769 - accuracy: 0.8081 - val_loss: 0.4200 - val_accuracy: 0.7789\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.87      0.77     10216\n",
      "           1       0.88      0.71      0.79     13579\n",
      "\n",
      "    accuracy                           0.78     23795\n",
      "   macro avg       0.79      0.79      0.78     23795\n",
      "weighted avg       0.80      0.78      0.78     23795\n",
      "\n",
      "network= DS_256 DS_512 BN DS_512 DS_256 BN DO DS_512 BN DO\n",
      "optimizer= Adagrad \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 8s 80us/step - loss: 0.4810 - accuracy: 0.7611 - val_loss: 0.4375 - val_accuracy: 0.7692\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 6s 67us/step - loss: 0.4350 - accuracy: 0.7735 - val_loss: 0.4258 - val_accuracy: 0.7746\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 7s 70us/step - loss: 0.4231 - accuracy: 0.7786 - val_loss: 0.4200 - val_accuracy: 0.7778\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 7s 69us/step - loss: 0.4190 - accuracy: 0.7800 - val_loss: 0.4155 - val_accuracy: 0.7812\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 6s 68us/step - loss: 0.4129 - accuracy: 0.7862 - val_loss: 0.4112 - val_accuracy: 0.7852\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 6s 67us/step - loss: 0.4091 - accuracy: 0.7880 - val_loss: 0.4104 - val_accuracy: 0.7848\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 6s 67us/step - loss: 0.4055 - accuracy: 0.7899 - val_loss: 0.4094 - val_accuracy: 0.7869\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 6s 67us/step - loss: 0.4023 - accuracy: 0.7923 - val_loss: 0.4051 - val_accuracy: 0.7866\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 6s 68us/step - loss: 0.3989 - accuracy: 0.7956 - val_loss: 0.4039 - val_accuracy: 0.7885\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 6s 68us/step - loss: 0.3977 - accuracy: 0.7951 - val_loss: 0.4048 - val_accuracy: 0.7889\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 6s 67us/step - loss: 0.3954 - accuracy: 0.7970 - val_loss: 0.4033 - val_accuracy: 0.7892\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 6s 67us/step - loss: 0.3932 - accuracy: 0.7992 - val_loss: 0.4018 - val_accuracy: 0.7908\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 6s 68us/step - loss: 0.3914 - accuracy: 0.7995 - val_loss: 0.4023 - val_accuracy: 0.7895\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 6s 68us/step - loss: 0.3901 - accuracy: 0.7992 - val_loss: 0.4060 - val_accuracy: 0.7878\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 6s 67us/step - loss: 0.3873 - accuracy: 0.8025 - val_loss: 0.4062 - val_accuracy: 0.7896\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.80      0.77     10216\n",
      "           1       0.84      0.78      0.81     13579\n",
      "\n",
      "    accuracy                           0.79     23795\n",
      "   macro avg       0.79      0.79      0.79     23795\n",
      "weighted avg       0.79      0.79      0.79     23795\n",
      "\n",
      "network= DS_256 DS_512 BN DS_512 DS_256 BN DO DS_512 BN DO\n",
      "optimizer= SGD \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 7s 74us/step - loss: 0.5088 - accuracy: 0.7432 - val_loss: 0.4489 - val_accuracy: 0.7709\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 6s 63us/step - loss: 0.4610 - accuracy: 0.7634 - val_loss: 0.4376 - val_accuracy: 0.7751\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 6s 63us/step - loss: 0.4475 - accuracy: 0.7705 - val_loss: 0.4388 - val_accuracy: 0.7725\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 6s 66us/step - loss: 0.4414 - accuracy: 0.7726 - val_loss: 0.4350 - val_accuracy: 0.7752\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 6s 64us/step - loss: 0.4386 - accuracy: 0.7745 - val_loss: 0.4328 - val_accuracy: 0.7771\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 6s 65us/step - loss: 0.4348 - accuracy: 0.7774 - val_loss: 0.4317 - val_accuracy: 0.7773\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 6s 65us/step - loss: 0.4341 - accuracy: 0.7773 - val_loss: 0.4324 - val_accuracy: 0.7764\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 6s 66us/step - loss: 0.4317 - accuracy: 0.7791 - val_loss: 0.4310 - val_accuracy: 0.7805\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 6s 64us/step - loss: 0.4304 - accuracy: 0.7790 - val_loss: 0.4312 - val_accuracy: 0.7768\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 6s 65us/step - loss: 0.4295 - accuracy: 0.7802 - val_loss: 0.4306 - val_accuracy: 0.7779\n",
      "Epoch 11/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95178/95178 [==============================] - 6s 65us/step - loss: 0.4290 - accuracy: 0.7809 - val_loss: 0.4300 - val_accuracy: 0.7795\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 6s 67us/step - loss: 0.4284 - accuracy: 0.7800 - val_loss: 0.4317 - val_accuracy: 0.7756\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 6s 68us/step - loss: 0.4274 - accuracy: 0.7805 - val_loss: 0.4301 - val_accuracy: 0.7773\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 6s 65us/step - loss: 0.4280 - accuracy: 0.7811 - val_loss: 0.4277 - val_accuracy: 0.7774\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 6s 65us/step - loss: 0.4276 - accuracy: 0.7798 - val_loss: 0.4291 - val_accuracy: 0.7778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.80      0.75     10216\n",
      "           1       0.83      0.76      0.80     13579\n",
      "\n",
      "    accuracy                           0.78     23795\n",
      "   macro avg       0.78      0.78      0.78     23795\n",
      "weighted avg       0.78      0.78      0.78     23795\n",
      "\n",
      "network= DL1256 DL1512 BN DL1512 DL1256 BN DO DL1512 BN DO\n",
      "optimizer= RMSprop \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fedor/.local/lib/python3.6/site-packages/keras/activations.py:235: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 7s 79us/step - loss: 0.4741 - accuracy: 0.7588 - val_loss: 0.4302 - val_accuracy: 0.7710\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 6s 66us/step - loss: 0.4235 - accuracy: 0.7801 - val_loss: 0.4201 - val_accuracy: 0.7833\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 6s 68us/step - loss: 0.4137 - accuracy: 0.7873 - val_loss: 0.4201 - val_accuracy: 0.7750\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 7s 69us/step - loss: 0.4050 - accuracy: 0.7939 - val_loss: 0.4114 - val_accuracy: 0.7875\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 7s 71us/step - loss: 0.3994 - accuracy: 0.7964 - val_loss: 0.4268 - val_accuracy: 0.7881\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 7s 71us/step - loss: 0.3939 - accuracy: 0.8025 - val_loss: 0.4095 - val_accuracy: 0.7892\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 7s 69us/step - loss: 0.3891 - accuracy: 0.8051 - val_loss: 0.4098 - val_accuracy: 0.7847\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 7s 69us/step - loss: 0.3844 - accuracy: 0.8090 - val_loss: 0.4104 - val_accuracy: 0.7871\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 6s 68us/step - loss: 0.3791 - accuracy: 0.8128 - val_loss: 0.4163 - val_accuracy: 0.7903\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 7s 69us/step - loss: 0.3751 - accuracy: 0.8165 - val_loss: 0.4127 - val_accuracy: 0.7878\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 7s 69us/step - loss: 0.3698 - accuracy: 0.8186 - val_loss: 0.4130 - val_accuracy: 0.7885\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 7s 69us/step - loss: 0.3642 - accuracy: 0.8231 - val_loss: 0.4265 - val_accuracy: 0.7883\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 7s 70us/step - loss: 0.3575 - accuracy: 0.8284 - val_loss: 0.4218 - val_accuracy: 0.7887\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 7s 70us/step - loss: 0.3513 - accuracy: 0.8317 - val_loss: 0.4292 - val_accuracy: 0.7844\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 7s 71us/step - loss: 0.3446 - accuracy: 0.8373 - val_loss: 0.4325 - val_accuracy: 0.7840\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.77      0.75     10216\n",
      "           1       0.82      0.79      0.81     13579\n",
      "\n",
      "    accuracy                           0.78     23795\n",
      "   macro avg       0.78      0.78      0.78     23795\n",
      "weighted avg       0.79      0.78      0.78     23795\n",
      "\n",
      "network= DL1256 DL1512 BN DL1512 DL1256 BN DO DL1512 BN DO\n",
      "optimizer= Adam \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 8s 87us/step - loss: 0.4699 - accuracy: 0.7589 - val_loss: 0.4289 - val_accuracy: 0.7744\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 7s 75us/step - loss: 0.4186 - accuracy: 0.7822 - val_loss: 0.4185 - val_accuracy: 0.7769\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 7s 76us/step - loss: 0.4070 - accuracy: 0.7902 - val_loss: 0.4105 - val_accuracy: 0.7848\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 7s 76us/step - loss: 0.3998 - accuracy: 0.7947 - val_loss: 0.4139 - val_accuracy: 0.7839\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 7s 77us/step - loss: 0.3920 - accuracy: 0.8006 - val_loss: 0.4061 - val_accuracy: 0.7884\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 7s 77us/step - loss: 0.3868 - accuracy: 0.8042 - val_loss: 0.4089 - val_accuracy: 0.7874\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 7s 77us/step - loss: 0.3804 - accuracy: 0.8076 - val_loss: 0.4085 - val_accuracy: 0.7895\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 7s 77us/step - loss: 0.3738 - accuracy: 0.8121 - val_loss: 0.4076 - val_accuracy: 0.7874\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 7s 77us/step - loss: 0.3674 - accuracy: 0.8171 - val_loss: 0.4193 - val_accuracy: 0.7871\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 7s 77us/step - loss: 0.3606 - accuracy: 0.8216 - val_loss: 0.4144 - val_accuracy: 0.7868\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 7s 77us/step - loss: 0.3513 - accuracy: 0.8276 - val_loss: 0.4177 - val_accuracy: 0.7856\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 7s 77us/step - loss: 0.3412 - accuracy: 0.8336 - val_loss: 0.4245 - val_accuracy: 0.7852\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 7s 76us/step - loss: 0.3318 - accuracy: 0.8391 - val_loss: 0.4468 - val_accuracy: 0.7821\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 7s 76us/step - loss: 0.3222 - accuracy: 0.8454 - val_loss: 0.4536 - val_accuracy: 0.7739\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 7s 77us/step - loss: 0.3100 - accuracy: 0.8518 - val_loss: 0.4693 - val_accuracy: 0.7776\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.77      0.75     10216\n",
      "           1       0.82      0.78      0.80     13579\n",
      "\n",
      "    accuracy                           0.78     23795\n",
      "   macro avg       0.77      0.78      0.77     23795\n",
      "weighted avg       0.78      0.78      0.78     23795\n",
      "\n",
      "network= DL1256 DL1512 BN DL1512 DL1256 BN DO DL1512 BN DO\n",
      "optimizer= Adadelta \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 9s 93us/step - loss: 0.4753 - accuracy: 0.7575 - val_loss: 0.4276 - val_accuracy: 0.7792\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 7s 78us/step - loss: 0.4186 - accuracy: 0.7819 - val_loss: 0.4171 - val_accuracy: 0.7835\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 8s 82us/step - loss: 0.4056 - accuracy: 0.7916 - val_loss: 0.4110 - val_accuracy: 0.7849\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 8s 82us/step - loss: 0.3967 - accuracy: 0.7972 - val_loss: 0.4106 - val_accuracy: 0.7870\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 8s 80us/step - loss: 0.3905 - accuracy: 0.8027 - val_loss: 0.4129 - val_accuracy: 0.7832\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 8s 81us/step - loss: 0.3830 - accuracy: 0.8069 - val_loss: 0.4115 - val_accuracy: 0.7875\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 8s 86us/step - loss: 0.3764 - accuracy: 0.8103 - val_loss: 0.4165 - val_accuracy: 0.7898\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 8s 83us/step - loss: 0.3682 - accuracy: 0.8150 - val_loss: 0.4183 - val_accuracy: 0.7899\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 8s 83us/step - loss: 0.3618 - accuracy: 0.8202 - val_loss: 0.4218 - val_accuracy: 0.7832\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 8s 83us/step - loss: 0.3542 - accuracy: 0.8262 - val_loss: 0.4317 - val_accuracy: 0.7864\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 8s 82us/step - loss: 0.3465 - accuracy: 0.8303 - val_loss: 0.4395 - val_accuracy: 0.7851\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 8s 82us/step - loss: 0.3381 - accuracy: 0.8358 - val_loss: 0.4475 - val_accuracy: 0.7834\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 8s 83us/step - loss: 0.3284 - accuracy: 0.8413 - val_loss: 0.4658 - val_accuracy: 0.7812\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 8s 82us/step - loss: 0.3178 - accuracy: 0.8477 - val_loss: 0.4583 - val_accuracy: 0.7801\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 8s 81us/step - loss: 0.3085 - accuracy: 0.8542 - val_loss: 0.4755 - val_accuracy: 0.7782\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.75      0.74     10216\n",
      "           1       0.81      0.80      0.80     13579\n",
      "\n",
      "    accuracy                           0.78     23795\n",
      "   macro avg       0.77      0.78      0.77     23795\n",
      "weighted avg       0.78      0.78      0.78     23795\n",
      "\n",
      "network= DL1256 DL1512 BN DL1512 DL1256 BN DO DL1512 BN DO\n",
      "optimizer= Nadam \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 9s 94us/step - loss: 0.4601 - accuracy: 0.7644 - val_loss: 0.4223 - val_accuracy: 0.7761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 7s 78us/step - loss: 0.4171 - accuracy: 0.7834 - val_loss: 0.4246 - val_accuracy: 0.7789\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 8s 83us/step - loss: 0.4068 - accuracy: 0.7892 - val_loss: 0.4126 - val_accuracy: 0.7797\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 8s 82us/step - loss: 0.4006 - accuracy: 0.7953 - val_loss: 0.4097 - val_accuracy: 0.7884\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 8s 82us/step - loss: 0.3934 - accuracy: 0.7985 - val_loss: 0.4042 - val_accuracy: 0.7895\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 8s 86us/step - loss: 0.3885 - accuracy: 0.8026 - val_loss: 0.4064 - val_accuracy: 0.7907\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 8s 87us/step - loss: 0.3826 - accuracy: 0.8063 - val_loss: 0.4072 - val_accuracy: 0.7865\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 8s 85us/step - loss: 0.3779 - accuracy: 0.8102 - val_loss: 0.4079 - val_accuracy: 0.7882\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 8s 81us/step - loss: 0.3716 - accuracy: 0.8133 - val_loss: 0.4110 - val_accuracy: 0.7880\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 8s 84us/step - loss: 0.3655 - accuracy: 0.8164 - val_loss: 0.4091 - val_accuracy: 0.7880\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 8s 83us/step - loss: 0.3577 - accuracy: 0.8219 - val_loss: 0.4167 - val_accuracy: 0.7876\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 8s 84us/step - loss: 0.3509 - accuracy: 0.8260 - val_loss: 0.4163 - val_accuracy: 0.7845\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 8s 81us/step - loss: 0.3424 - accuracy: 0.8331 - val_loss: 0.4269 - val_accuracy: 0.7855\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 8s 82us/step - loss: 0.3340 - accuracy: 0.8371 - val_loss: 0.4365 - val_accuracy: 0.7862\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 8s 84us/step - loss: 0.3244 - accuracy: 0.8429 - val_loss: 0.4400 - val_accuracy: 0.7806\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.75      0.75     10216\n",
      "           1       0.81      0.80      0.81     13579\n",
      "\n",
      "    accuracy                           0.78     23795\n",
      "   macro avg       0.78      0.78      0.78     23795\n",
      "weighted avg       0.78      0.78      0.78     23795\n",
      "\n",
      "network= DL1256 DL1512 BN DL1512 DL1256 BN DO DL1512 BN DO\n",
      "optimizer= Adamax \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 8s 88us/step - loss: 0.4755 - accuracy: 0.7575 - val_loss: 0.4286 - val_accuracy: 0.7744\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 7s 77us/step - loss: 0.4212 - accuracy: 0.7807 - val_loss: 0.4209 - val_accuracy: 0.7765\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 7s 78us/step - loss: 0.4085 - accuracy: 0.7895 - val_loss: 0.4135 - val_accuracy: 0.7833\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 7s 76us/step - loss: 0.3996 - accuracy: 0.7955 - val_loss: 0.4098 - val_accuracy: 0.7889\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 7s 77us/step - loss: 0.3899 - accuracy: 0.8006 - val_loss: 0.4069 - val_accuracy: 0.7878\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 7s 76us/step - loss: 0.3817 - accuracy: 0.8079 - val_loss: 0.4105 - val_accuracy: 0.7893\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 7s 76us/step - loss: 0.3725 - accuracy: 0.8121 - val_loss: 0.4213 - val_accuracy: 0.7854\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 8s 79us/step - loss: 0.3626 - accuracy: 0.8201 - val_loss: 0.4135 - val_accuracy: 0.7857\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 8s 81us/step - loss: 0.3543 - accuracy: 0.8237 - val_loss: 0.4228 - val_accuracy: 0.7891\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 8s 79us/step - loss: 0.3417 - accuracy: 0.8318 - val_loss: 0.4330 - val_accuracy: 0.7822\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 7s 77us/step - loss: 0.3307 - accuracy: 0.8387 - val_loss: 0.4503 - val_accuracy: 0.7786\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 7s 76us/step - loss: 0.3170 - accuracy: 0.8463 - val_loss: 0.4520 - val_accuracy: 0.7815\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 7s 78us/step - loss: 0.3034 - accuracy: 0.8539 - val_loss: 0.4872 - val_accuracy: 0.7791\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 7s 78us/step - loss: 0.2891 - accuracy: 0.8627 - val_loss: 0.4883 - val_accuracy: 0.7744\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 7s 76us/step - loss: 0.2746 - accuracy: 0.8703 - val_loss: 0.5280 - val_accuracy: 0.7723\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.74      0.74     10216\n",
      "           1       0.80      0.80      0.80     13579\n",
      "\n",
      "    accuracy                           0.77     23795\n",
      "   macro avg       0.77      0.77      0.77     23795\n",
      "weighted avg       0.77      0.77      0.77     23795\n",
      "\n",
      "network= DL1256 DL1512 BN DL1512 DL1256 BN DO DL1512 BN DO\n",
      "optimizer= Adagrad \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 8s 81us/step - loss: 0.4662 - accuracy: 0.7638 - val_loss: 0.4266 - val_accuracy: 0.7778\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 6s 67us/step - loss: 0.4158 - accuracy: 0.7837 - val_loss: 0.4163 - val_accuracy: 0.7802\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 6s 67us/step - loss: 0.4017 - accuracy: 0.7920 - val_loss: 0.4165 - val_accuracy: 0.7833\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 7s 70us/step - loss: 0.3909 - accuracy: 0.7986 - val_loss: 0.4139 - val_accuracy: 0.7835\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 7s 71us/step - loss: 0.3825 - accuracy: 0.8050 - val_loss: 0.4143 - val_accuracy: 0.7852\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 7s 69us/step - loss: 0.3736 - accuracy: 0.8105 - val_loss: 0.4165 - val_accuracy: 0.7837\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 7s 70us/step - loss: 0.3647 - accuracy: 0.8169 - val_loss: 0.4210 - val_accuracy: 0.7823\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 7s 69us/step - loss: 0.3552 - accuracy: 0.8222 - val_loss: 0.4271 - val_accuracy: 0.7821\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 7s 69us/step - loss: 0.3460 - accuracy: 0.8280 - val_loss: 0.4251 - val_accuracy: 0.7802\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 7s 70us/step - loss: 0.3366 - accuracy: 0.8338 - val_loss: 0.4417 - val_accuracy: 0.7803\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 7s 70us/step - loss: 0.3267 - accuracy: 0.8397 - val_loss: 0.4525 - val_accuracy: 0.7781\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 7s 69us/step - loss: 0.3165 - accuracy: 0.8459 - val_loss: 0.4638 - val_accuracy: 0.7715\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 7s 70us/step - loss: 0.3054 - accuracy: 0.8517 - val_loss: 0.4706 - val_accuracy: 0.7704\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 7s 69us/step - loss: 0.2938 - accuracy: 0.8585 - val_loss: 0.4854 - val_accuracy: 0.7657\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 7s 70us/step - loss: 0.2843 - accuracy: 0.8641 - val_loss: 0.5040 - val_accuracy: 0.7722\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.76      0.74     10216\n",
      "           1       0.81      0.78      0.80     13579\n",
      "\n",
      "    accuracy                           0.77     23795\n",
      "   macro avg       0.77      0.77      0.77     23795\n",
      "weighted avg       0.77      0.77      0.77     23795\n",
      "\n",
      "network= DL1256 DL1512 BN DL1512 DL1256 BN DO DL1512 BN DO\n",
      "optimizer= SGD \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 8s 81us/step - loss: 0.5267 - accuracy: 0.7365 - val_loss: 0.4498 - val_accuracy: 0.7696\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 6s 67us/step - loss: 0.4553 - accuracy: 0.7672 - val_loss: 0.4361 - val_accuracy: 0.7753\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95178/95178 [==============================] - 7s 69us/step - loss: 0.4333 - accuracy: 0.7789 - val_loss: 0.4316 - val_accuracy: 0.7753\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 7s 76us/step - loss: 0.4220 - accuracy: 0.7839 - val_loss: 0.4281 - val_accuracy: 0.7788\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 7s 71us/step - loss: 0.4143 - accuracy: 0.7890 - val_loss: 0.4260 - val_accuracy: 0.7791\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 7s 73us/step - loss: 0.4070 - accuracy: 0.7937 - val_loss: 0.4235 - val_accuracy: 0.7811\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 7s 71us/step - loss: 0.4040 - accuracy: 0.7946 - val_loss: 0.4239 - val_accuracy: 0.7792\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 7s 70us/step - loss: 0.3977 - accuracy: 0.7989 - val_loss: 0.4208 - val_accuracy: 0.7813\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 7s 71us/step - loss: 0.3923 - accuracy: 0.8037 - val_loss: 0.4215 - val_accuracy: 0.7806\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 7s 69us/step - loss: 0.3870 - accuracy: 0.8065 - val_loss: 0.4228 - val_accuracy: 0.7797\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 6s 68us/step - loss: 0.3839 - accuracy: 0.8082 - val_loss: 0.4243 - val_accuracy: 0.7806\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 6s 68us/step - loss: 0.3776 - accuracy: 0.8116 - val_loss: 0.4237 - val_accuracy: 0.7815\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 7s 69us/step - loss: 0.3744 - accuracy: 0.8142 - val_loss: 0.4256 - val_accuracy: 0.7800\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 6s 67us/step - loss: 0.3693 - accuracy: 0.8178 - val_loss: 0.4269 - val_accuracy: 0.7789\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 7s 69us/step - loss: 0.3644 - accuracy: 0.8200 - val_loss: 0.4298 - val_accuracy: 0.7762\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.76      0.74     10216\n",
      "           1       0.81      0.79      0.80     13579\n",
      "\n",
      "    accuracy                           0.78     23795\n",
      "   macro avg       0.77      0.77      0.77     23795\n",
      "weighted avg       0.78      0.78      0.78     23795\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for layers in MODELS:\n",
    "  for optimizer in OPTIMIZERS:\n",
    "    print('network=', layers)\n",
    "    print('optimizer=', optimizer, '\\n')\n",
    "    model = make_NN('binary_crossentropy', optimizer, layers, input_dim=X_train.shape[1])\n",
    "    model.fit(X_train, y_train, epochs=15, batch_size=128, validation_data=(X_val, y_val))\n",
    "    y_pred = np.rint(model.predict(X_val)).astype(int)\n",
    "    print(classification_report(y_val, y_pred))\n",
    "    results.append([optimizer, layers, accuracy_score(y_val, y_pred)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(results, columns = ['optimizer', 'layers', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = res_df.sort_values('accuracy', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>optimizer</th>\n",
       "      <th>layers</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>DS_256 DS_128 BN DO DS_64 DS_32 BN DO DS_16 DS_8</td>\n",
       "      <td>0.790628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>DS_256 DS_128 BN DO DS_64 DS_32 BN DO DS_16 DS_8</td>\n",
       "      <td>0.790502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>DS_256 DS_512 BN DS_512 DS_256 BN DO DS_512 BN DO</td>\n",
       "      <td>0.789578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>DS_128 DS_64 BN DO DS_32 DS_8</td>\n",
       "      <td>0.789073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>DS_128 DS_64 BN DO DS_32 DS_8</td>\n",
       "      <td>0.788947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>DS_256 DS_512 BN DS_512 DS_256 BN DO DS_512 BN DO</td>\n",
       "      <td>0.787981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>DS_256 DS_128 BN DO DS_64 DS_32 BN DO DS_16 DS_8</td>\n",
       "      <td>0.787602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>DS_256 DS_512 BN DS_512 DS_256 BN DO DS_512 BN DO</td>\n",
       "      <td>0.787602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>DS_128 DS_64 BN DO DS_32 DS_8</td>\n",
       "      <td>0.787350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Adam</td>\n",
       "      <td>DS_256 DS_128 BN DO DS_64 DS_32 BN DO DS_16 DS_8</td>\n",
       "      <td>0.786132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>DS_256 DS_128 BN DO DS_64 DS_32 BN DO DS_16 DS_8</td>\n",
       "      <td>0.786132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>DS_128 DS_64 BN DO DS_32 DS_8</td>\n",
       "      <td>0.786132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>Adam</td>\n",
       "      <td>DS_256 DS_512 BN DS_512 DS_256 BN DO DS_512 BN DO</td>\n",
       "      <td>0.785753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>DS_256 DS_512 BN DS_512 DS_256 BN DO DS_512 BN DO</td>\n",
       "      <td>0.785627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>DS_128 DS_64 BN DO DS_32 DS_8</td>\n",
       "      <td>0.784114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>DS_128 DS_64 BN DO DS_32 DS_8</td>\n",
       "      <td>0.784072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>DL1256 DL1512 BN DL1512 DL1256 BN DO DL1512 BN DO</td>\n",
       "      <td>0.784030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>DS_256 DS_128 BN DO DS_64 DS_32 BN DO DS_16 DS_8</td>\n",
       "      <td>0.781383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>DL1256 DL1512 BN DL1512 DL1256 BN DO DL1512 BN DO</td>\n",
       "      <td>0.780626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>SGD</td>\n",
       "      <td>DS_256 DS_128 BN DO DS_64 DS_32 BN DO DS_16 DS_8</td>\n",
       "      <td>0.779113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>DS_256 DS_512 BN DS_512 DS_256 BN DO DS_512 BN DO</td>\n",
       "      <td>0.778903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>DL1256 DL1512 BN DL1512 DL1256 BN DO DL1512 BN DO</td>\n",
       "      <td>0.778189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>SGD</td>\n",
       "      <td>DS_128 DS_64 BN DO DS_32 DS_8</td>\n",
       "      <td>0.777895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>SGD</td>\n",
       "      <td>DS_256 DS_512 BN DS_512 DS_256 BN DO DS_512 BN DO</td>\n",
       "      <td>0.777768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>Adam</td>\n",
       "      <td>DL1256 DL1512 BN DL1512 DL1256 BN DO DL1512 BN DO</td>\n",
       "      <td>0.777600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>DR_256 DR_512 BN DR_512 DR_256 BN DO DR_512 BN DO</td>\n",
       "      <td>0.776718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>SGD</td>\n",
       "      <td>DL1256 DL1512 BN DL1512 DL1256 BN DO DL1512 BN DO</td>\n",
       "      <td>0.776213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>SGD</td>\n",
       "      <td>DR_256 DR_512 BN DR_512 DR_256 BN DO DR_512 BN DO</td>\n",
       "      <td>0.774238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>Adam</td>\n",
       "      <td>DR_256 DR_512 BN DR_512 DR_256 BN DO DR_512 BN DO</td>\n",
       "      <td>0.773944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>DL1256 DL1512 BN DL1512 DL1256 BN DO DL1512 BN DO</td>\n",
       "      <td>0.772305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>DL1256 DL1512 BN DL1512 DL1256 BN DO DL1512 BN DO</td>\n",
       "      <td>0.772221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>DR_256 DR_512 BN DR_512 DR_256 BN DO DR_512 BN DO</td>\n",
       "      <td>0.772137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>DR_256 DR_512 BN DR_512 DR_256 BN DO DR_512 BN DO</td>\n",
       "      <td>0.770834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>DR_256 DR_512 BN DR_512 DR_256 BN DO DR_512 BN DO</td>\n",
       "      <td>0.761462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>DR_256 DR_512 BN DR_512 DR_256 BN DO DR_512 BN DO</td>\n",
       "      <td>0.761084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   optimizer                                             layers  accuracy\n",
       "11    Adamax   DS_256 DS_128 BN DO DS_64 DS_32 BN DO DS_16 DS_8  0.790628\n",
       "9   Adadelta   DS_256 DS_128 BN DO DS_64 DS_32 BN DO DS_16 DS_8  0.790502\n",
       "26   Adagrad  DS_256 DS_512 BN DS_512 DS_256 BN DO DS_512 BN DO  0.789578\n",
       "0    RMSprop                      DS_128 DS_64 BN DO DS_32 DS_8  0.789073\n",
       "4     Adamax                      DS_128 DS_64 BN DO DS_32 DS_8  0.788947\n",
       "21   RMSprop  DS_256 DS_512 BN DS_512 DS_256 BN DO DS_512 BN DO  0.787981\n",
       "7    RMSprop   DS_256 DS_128 BN DO DS_64 DS_32 BN DO DS_16 DS_8  0.787602\n",
       "23  Adadelta  DS_256 DS_512 BN DS_512 DS_256 BN DO DS_512 BN DO  0.787602\n",
       "2   Adadelta                      DS_128 DS_64 BN DO DS_32 DS_8  0.787350\n",
       "8       Adam   DS_256 DS_128 BN DO DS_64 DS_32 BN DO DS_16 DS_8  0.786132\n",
       "12   Adagrad   DS_256 DS_128 BN DO DS_64 DS_32 BN DO DS_16 DS_8  0.786132\n",
       "1       Adam                      DS_128 DS_64 BN DO DS_32 DS_8  0.786132\n",
       "22      Adam  DS_256 DS_512 BN DS_512 DS_256 BN DO DS_512 BN DO  0.785753\n",
       "24     Nadam  DS_256 DS_512 BN DS_512 DS_256 BN DO DS_512 BN DO  0.785627\n",
       "5    Adagrad                      DS_128 DS_64 BN DO DS_32 DS_8  0.784114\n",
       "3      Nadam                      DS_128 DS_64 BN DO DS_32 DS_8  0.784072\n",
       "28   RMSprop  DL1256 DL1512 BN DL1512 DL1256 BN DO DL1512 BN DO  0.784030\n",
       "10     Nadam   DS_256 DS_128 BN DO DS_64 DS_32 BN DO DS_16 DS_8  0.781383\n",
       "31     Nadam  DL1256 DL1512 BN DL1512 DL1256 BN DO DL1512 BN DO  0.780626\n",
       "13       SGD   DS_256 DS_128 BN DO DS_64 DS_32 BN DO DS_16 DS_8  0.779113\n",
       "25    Adamax  DS_256 DS_512 BN DS_512 DS_256 BN DO DS_512 BN DO  0.778903\n",
       "30  Adadelta  DL1256 DL1512 BN DL1512 DL1256 BN DO DL1512 BN DO  0.778189\n",
       "6        SGD                      DS_128 DS_64 BN DO DS_32 DS_8  0.777895\n",
       "27       SGD  DS_256 DS_512 BN DS_512 DS_256 BN DO DS_512 BN DO  0.777768\n",
       "29      Adam  DL1256 DL1512 BN DL1512 DL1256 BN DO DL1512 BN DO  0.777600\n",
       "14   RMSprop  DR_256 DR_512 BN DR_512 DR_256 BN DO DR_512 BN DO  0.776718\n",
       "34       SGD  DL1256 DL1512 BN DL1512 DL1256 BN DO DL1512 BN DO  0.776213\n",
       "20       SGD  DR_256 DR_512 BN DR_512 DR_256 BN DO DR_512 BN DO  0.774238\n",
       "15      Adam  DR_256 DR_512 BN DR_512 DR_256 BN DO DR_512 BN DO  0.773944\n",
       "32    Adamax  DL1256 DL1512 BN DL1512 DL1256 BN DO DL1512 BN DO  0.772305\n",
       "33   Adagrad  DL1256 DL1512 BN DL1512 DL1256 BN DO DL1512 BN DO  0.772221\n",
       "16  Adadelta  DR_256 DR_512 BN DR_512 DR_256 BN DO DR_512 BN DO  0.772137\n",
       "17     Nadam  DR_256 DR_512 BN DR_512 DR_256 BN DO DR_512 BN DO  0.770834\n",
       "19   Adagrad  DR_256 DR_512 BN DR_512 DR_256 BN DO DR_512 BN DO  0.761462\n",
       "18    Adamax  DR_256 DR_512 BN DR_512 DR_256 BN DO DR_512 BN DO  0.761084"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "    'DS_256 DS_128 BN DO DS_64 DS_32 BN DO DS_16 DS_8',\n",
    "    'DS_512 DS_256 BN DO DS_128 DS_64 BN DO DS_32 DS_16',\n",
    "    'DS_256 DS_128 BN DO DS_128 DS_64 BN DO DS_32 DS_16',\n",
    "]\n",
    "OPTIMIZERS = [\n",
    "    'Adamax'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network= DS_256 DS_128 BN DO DS_64 DS_32 BN DO DS_16 DS_8\n",
      "optimizer= Adamax \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/30\n",
      "95178/95178 [==============================] - 4s 38us/step - loss: 0.5322 - accuracy: 0.7621 - val_loss: 0.4984 - val_accuracy: 0.7708\n",
      "Epoch 2/30\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.4634 - accuracy: 0.7789 - val_loss: 0.4542 - val_accuracy: 0.7783\n",
      "Epoch 3/30\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.4454 - accuracy: 0.7810 - val_loss: 0.4409 - val_accuracy: 0.7808\n",
      "Epoch 4/30\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.4343 - accuracy: 0.7831 - val_loss: 0.4319 - val_accuracy: 0.7803\n",
      "Epoch 5/30\n",
      "95178/95178 [==============================] - 2s 21us/step - loss: 0.4263 - accuracy: 0.7838 - val_loss: 0.4256 - val_accuracy: 0.7827\n",
      "Epoch 6/30\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.4184 - accuracy: 0.7877 - val_loss: 0.4199 - val_accuracy: 0.7850\n",
      "Epoch 7/30\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.4134 - accuracy: 0.7898 - val_loss: 0.4162 - val_accuracy: 0.7846\n",
      "Epoch 8/30\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.4083 - accuracy: 0.7929 - val_loss: 0.4139 - val_accuracy: 0.7880\n",
      "Epoch 9/30\n",
      "95178/95178 [==============================] - 2s 21us/step - loss: 0.4056 - accuracy: 0.7937 - val_loss: 0.4167 - val_accuracy: 0.7885\n",
      "Epoch 10/30\n",
      "95178/95178 [==============================] - 2s 21us/step - loss: 0.4005 - accuracy: 0.7972 - val_loss: 0.4091 - val_accuracy: 0.7869\n",
      "Epoch 11/30\n",
      "95178/95178 [==============================] - 2s 21us/step - loss: 0.3981 - accuracy: 0.7978 - val_loss: 0.4081 - val_accuracy: 0.7866\n",
      "Epoch 12/30\n",
      "95178/95178 [==============================] - 2s 21us/step - loss: 0.3949 - accuracy: 0.8014 - val_loss: 0.4096 - val_accuracy: 0.7862\n",
      "Epoch 13/30\n",
      "95178/95178 [==============================] - 2s 21us/step - loss: 0.3933 - accuracy: 0.8019 - val_loss: 0.4100 - val_accuracy: 0.7871\n",
      "Epoch 14/30\n",
      "95178/95178 [==============================] - 2s 21us/step - loss: 0.3897 - accuracy: 0.8025 - val_loss: 0.4097 - val_accuracy: 0.7881\n",
      "Epoch 15/30\n",
      "95178/95178 [==============================] - 2s 21us/step - loss: 0.3882 - accuracy: 0.8045 - val_loss: 0.4114 - val_accuracy: 0.7869\n",
      "Epoch 16/30\n",
      "95178/95178 [==============================] - 2s 21us/step - loss: 0.3845 - accuracy: 0.8072 - val_loss: 0.4089 - val_accuracy: 0.7887\n",
      "Epoch 17/30\n",
      "95178/95178 [==============================] - 2s 21us/step - loss: 0.3827 - accuracy: 0.8067 - val_loss: 0.4072 - val_accuracy: 0.7858\n",
      "Epoch 18/30\n",
      "95178/95178 [==============================] - 2s 21us/step - loss: 0.3797 - accuracy: 0.8101 - val_loss: 0.4068 - val_accuracy: 0.7875\n",
      "Epoch 19/30\n",
      "95178/95178 [==============================] - 2s 21us/step - loss: 0.3771 - accuracy: 0.8105 - val_loss: 0.4130 - val_accuracy: 0.7845\n",
      "Epoch 20/30\n",
      "95178/95178 [==============================] - 2s 21us/step - loss: 0.3742 - accuracy: 0.8135 - val_loss: 0.4121 - val_accuracy: 0.7855\n",
      "Epoch 21/30\n",
      "95178/95178 [==============================] - 2s 21us/step - loss: 0.3726 - accuracy: 0.8147 - val_loss: 0.4121 - val_accuracy: 0.7879\n",
      "Epoch 22/30\n",
      "95178/95178 [==============================] - 2s 21us/step - loss: 0.3688 - accuracy: 0.8171 - val_loss: 0.4104 - val_accuracy: 0.7876\n",
      "Epoch 23/30\n",
      "95178/95178 [==============================] - 2s 21us/step - loss: 0.3679 - accuracy: 0.8176 - val_loss: 0.4117 - val_accuracy: 0.7858\n",
      "Epoch 24/30\n",
      "95178/95178 [==============================] - 2s 21us/step - loss: 0.3648 - accuracy: 0.8201 - val_loss: 0.4175 - val_accuracy: 0.7861\n",
      "Epoch 25/30\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.3625 - accuracy: 0.8211 - val_loss: 0.4174 - val_accuracy: 0.7855\n",
      "Epoch 26/30\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.3596 - accuracy: 0.8239 - val_loss: 0.4196 - val_accuracy: 0.7831\n",
      "Epoch 27/30\n",
      "95178/95178 [==============================] - 2s 21us/step - loss: 0.3564 - accuracy: 0.8255 - val_loss: 0.4247 - val_accuracy: 0.7866\n",
      "Epoch 28/30\n",
      "95178/95178 [==============================] - 2s 21us/step - loss: 0.3530 - accuracy: 0.8283 - val_loss: 0.4211 - val_accuracy: 0.7883\n",
      "Epoch 29/30\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.3497 - accuracy: 0.8300 - val_loss: 0.4234 - val_accuracy: 0.7847\n",
      "Epoch 30/30\n",
      "95178/95178 [==============================] - 2s 21us/step - loss: 0.3472 - accuracy: 0.8317 - val_loss: 0.4280 - val_accuracy: 0.7864\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.76      0.75     10216\n",
      "           1       0.82      0.80      0.81     13579\n",
      "\n",
      "    accuracy                           0.79     23795\n",
      "   macro avg       0.78      0.78      0.78     23795\n",
      "weighted avg       0.79      0.79      0.79     23795\n",
      "\n",
      "network= DS_512 DS_256 BN DO DS_128 DS_64 BN DO DS_32 DS_16\n",
      "optimizer= Adamax \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/30\n",
      "95178/95178 [==============================] - 4s 46us/step - loss: 0.5106 - accuracy: 0.7604 - val_loss: 0.4849 - val_accuracy: 0.7716\n",
      "Epoch 2/30\n",
      "95178/95178 [==============================] - 3s 29us/step - loss: 0.4476 - accuracy: 0.7787 - val_loss: 0.4406 - val_accuracy: 0.7770\n",
      "Epoch 3/30\n",
      "95178/95178 [==============================] - 3s 29us/step - loss: 0.4287 - accuracy: 0.7820 - val_loss: 0.4297 - val_accuracy: 0.7799\n",
      "Epoch 4/30\n",
      "95178/95178 [==============================] - 3s 29us/step - loss: 0.4185 - accuracy: 0.7864 - val_loss: 0.4189 - val_accuracy: 0.7852\n",
      "Epoch 5/30\n",
      "95178/95178 [==============================] - 3s 31us/step - loss: 0.4115 - accuracy: 0.7891 - val_loss: 0.4124 - val_accuracy: 0.7860\n",
      "Epoch 6/30\n",
      "95178/95178 [==============================] - 3s 29us/step - loss: 0.4060 - accuracy: 0.7916 - val_loss: 0.4172 - val_accuracy: 0.7866\n",
      "Epoch 7/30\n",
      "95178/95178 [==============================] - 3s 30us/step - loss: 0.4001 - accuracy: 0.7969 - val_loss: 0.4082 - val_accuracy: 0.7879\n",
      "Epoch 8/30\n",
      "95178/95178 [==============================] - 3s 30us/step - loss: 0.3973 - accuracy: 0.7970 - val_loss: 0.4040 - val_accuracy: 0.7897\n",
      "Epoch 9/30\n",
      "95178/95178 [==============================] - 3s 30us/step - loss: 0.3933 - accuracy: 0.7993 - val_loss: 0.4056 - val_accuracy: 0.7888\n",
      "Epoch 10/30\n",
      "95178/95178 [==============================] - 3s 30us/step - loss: 0.3899 - accuracy: 0.8009 - val_loss: 0.4137 - val_accuracy: 0.7876\n",
      "Epoch 11/30\n",
      "95178/95178 [==============================] - 3s 30us/step - loss: 0.3877 - accuracy: 0.8030 - val_loss: 0.4054 - val_accuracy: 0.7912\n",
      "Epoch 12/30\n",
      "95178/95178 [==============================] - 3s 30us/step - loss: 0.3849 - accuracy: 0.8049 - val_loss: 0.4090 - val_accuracy: 0.7852\n",
      "Epoch 13/30\n",
      "95178/95178 [==============================] - 3s 30us/step - loss: 0.3821 - accuracy: 0.8068 - val_loss: 0.4070 - val_accuracy: 0.7915\n",
      "Epoch 14/30\n",
      "95178/95178 [==============================] - 3s 31us/step - loss: 0.3785 - accuracy: 0.8082 - val_loss: 0.4047 - val_accuracy: 0.7874\n",
      "Epoch 15/30\n",
      "95178/95178 [==============================] - 3s 30us/step - loss: 0.3759 - accuracy: 0.8103 - val_loss: 0.4076 - val_accuracy: 0.7898\n",
      "Epoch 16/30\n",
      "95178/95178 [==============================] - 3s 30us/step - loss: 0.3721 - accuracy: 0.8135 - val_loss: 0.4119 - val_accuracy: 0.7892\n",
      "Epoch 17/30\n",
      "95178/95178 [==============================] - 3s 29us/step - loss: 0.3702 - accuracy: 0.8162 - val_loss: 0.4083 - val_accuracy: 0.7902\n",
      "Epoch 18/30\n",
      "95178/95178 [==============================] - 3s 29us/step - loss: 0.3662 - accuracy: 0.8178 - val_loss: 0.4117 - val_accuracy: 0.7897\n",
      "Epoch 19/30\n",
      "95178/95178 [==============================] - 3s 29us/step - loss: 0.3643 - accuracy: 0.8187 - val_loss: 0.4220 - val_accuracy: 0.7897\n",
      "Epoch 20/30\n",
      "95178/95178 [==============================] - 3s 29us/step - loss: 0.3607 - accuracy: 0.8219 - val_loss: 0.4135 - val_accuracy: 0.7881\n",
      "Epoch 21/30\n",
      "95178/95178 [==============================] - 3s 29us/step - loss: 0.3561 - accuracy: 0.8253 - val_loss: 0.4190 - val_accuracy: 0.7861\n",
      "Epoch 22/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95178/95178 [==============================] - 3s 30us/step - loss: 0.3534 - accuracy: 0.8261 - val_loss: 0.4193 - val_accuracy: 0.7855\n",
      "Epoch 23/30\n",
      "95178/95178 [==============================] - 3s 29us/step - loss: 0.3492 - accuracy: 0.8301 - val_loss: 0.4219 - val_accuracy: 0.7854\n",
      "Epoch 24/30\n",
      "95178/95178 [==============================] - 3s 29us/step - loss: 0.3468 - accuracy: 0.8323 - val_loss: 0.4242 - val_accuracy: 0.7828\n",
      "Epoch 25/30\n",
      "95178/95178 [==============================] - 3s 29us/step - loss: 0.3427 - accuracy: 0.8347 - val_loss: 0.4215 - val_accuracy: 0.7843\n",
      "Epoch 26/30\n",
      "95178/95178 [==============================] - 3s 29us/step - loss: 0.3384 - accuracy: 0.8361 - val_loss: 0.4365 - val_accuracy: 0.7826\n",
      "Epoch 27/30\n",
      "95178/95178 [==============================] - 3s 29us/step - loss: 0.3357 - accuracy: 0.8374 - val_loss: 0.4354 - val_accuracy: 0.7821\n",
      "Epoch 28/30\n",
      "95178/95178 [==============================] - 3s 29us/step - loss: 0.3324 - accuracy: 0.8401 - val_loss: 0.4427 - val_accuracy: 0.7755\n",
      "Epoch 29/30\n",
      "95178/95178 [==============================] - 3s 29us/step - loss: 0.3287 - accuracy: 0.8419 - val_loss: 0.4315 - val_accuracy: 0.7831\n",
      "Epoch 30/30\n",
      "95178/95178 [==============================] - 3s 29us/step - loss: 0.3237 - accuracy: 0.8446 - val_loss: 0.4447 - val_accuracy: 0.7812\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75     10216\n",
      "           1       0.81      0.81      0.81     13579\n",
      "\n",
      "    accuracy                           0.78     23795\n",
      "   macro avg       0.78      0.78      0.78     23795\n",
      "weighted avg       0.78      0.78      0.78     23795\n",
      "\n",
      "network= DS_256 DS_128 BN DO DS_128 DS_64 BN DO DS_32 DS_16\n",
      "optimizer= Adamax \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/30\n",
      "95178/95178 [==============================] - 4s 39us/step - loss: 0.4978 - accuracy: 0.7616 - val_loss: 0.4871 - val_accuracy: 0.7726\n",
      "Epoch 2/30\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4497 - accuracy: 0.7792 - val_loss: 0.4434 - val_accuracy: 0.7758\n",
      "Epoch 3/30\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4338 - accuracy: 0.7810 - val_loss: 0.4315 - val_accuracy: 0.7779\n",
      "Epoch 4/30\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.4234 - accuracy: 0.7832 - val_loss: 0.4244 - val_accuracy: 0.7799\n",
      "Epoch 5/30\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4173 - accuracy: 0.7848 - val_loss: 0.4192 - val_accuracy: 0.7845\n",
      "Epoch 6/30\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4122 - accuracy: 0.7878 - val_loss: 0.4170 - val_accuracy: 0.7800\n",
      "Epoch 7/30\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4087 - accuracy: 0.7893 - val_loss: 0.4141 - val_accuracy: 0.7840\n",
      "Epoch 8/30\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4052 - accuracy: 0.7908 - val_loss: 0.4128 - val_accuracy: 0.7858\n",
      "Epoch 9/30\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4026 - accuracy: 0.7926 - val_loss: 0.4163 - val_accuracy: 0.7824\n",
      "Epoch 10/30\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3994 - accuracy: 0.7941 - val_loss: 0.4076 - val_accuracy: 0.7879\n",
      "Epoch 11/30\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3962 - accuracy: 0.7960 - val_loss: 0.4096 - val_accuracy: 0.7835\n",
      "Epoch 12/30\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3932 - accuracy: 0.7974 - val_loss: 0.4048 - val_accuracy: 0.7908\n",
      "Epoch 13/30\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3913 - accuracy: 0.7998 - val_loss: 0.4034 - val_accuracy: 0.7902\n",
      "Epoch 14/30\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3894 - accuracy: 0.8005 - val_loss: 0.4045 - val_accuracy: 0.7895\n",
      "Epoch 15/30\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3872 - accuracy: 0.8031 - val_loss: 0.4037 - val_accuracy: 0.7876\n",
      "Epoch 16/30\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3844 - accuracy: 0.8042 - val_loss: 0.4064 - val_accuracy: 0.7894\n",
      "Epoch 17/30\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3820 - accuracy: 0.8069 - val_loss: 0.4043 - val_accuracy: 0.7891\n",
      "Epoch 18/30\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3805 - accuracy: 0.8072 - val_loss: 0.4050 - val_accuracy: 0.7890\n",
      "Epoch 19/30\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3782 - accuracy: 0.8086 - val_loss: 0.4088 - val_accuracy: 0.7895\n",
      "Epoch 20/30\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3761 - accuracy: 0.8101 - val_loss: 0.4073 - val_accuracy: 0.7891\n",
      "Epoch 21/30\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3734 - accuracy: 0.8123 - val_loss: 0.4086 - val_accuracy: 0.7874\n",
      "Epoch 22/30\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3710 - accuracy: 0.8142 - val_loss: 0.4175 - val_accuracy: 0.7875\n",
      "Epoch 23/30\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3691 - accuracy: 0.8156 - val_loss: 0.4154 - val_accuracy: 0.7896\n",
      "Epoch 24/30\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3662 - accuracy: 0.8170 - val_loss: 0.4111 - val_accuracy: 0.7855\n",
      "Epoch 25/30\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3634 - accuracy: 0.8185 - val_loss: 0.4145 - val_accuracy: 0.7881\n",
      "Epoch 26/30\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3614 - accuracy: 0.8212 - val_loss: 0.4128 - val_accuracy: 0.7898\n",
      "Epoch 27/30\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3580 - accuracy: 0.8218 - val_loss: 0.4135 - val_accuracy: 0.7868\n",
      "Epoch 28/30\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3550 - accuracy: 0.8240 - val_loss: 0.4162 - val_accuracy: 0.7871\n",
      "Epoch 29/30\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3542 - accuracy: 0.8251 - val_loss: 0.4203 - val_accuracy: 0.7851\n",
      "Epoch 30/30\n",
      "95178/95178 [==============================] - 2s 24us/step - loss: 0.3503 - accuracy: 0.8269 - val_loss: 0.4209 - val_accuracy: 0.7863\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75     10216\n",
      "           1       0.81      0.81      0.81     13579\n",
      "\n",
      "    accuracy                           0.79     23795\n",
      "   macro avg       0.78      0.78      0.78     23795\n",
      "weighted avg       0.79      0.79      0.79     23795\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for layers in MODELS:\n",
    "  for optimizer in OPTIMIZERS:\n",
    "    print('network=', layers)\n",
    "    print('optimizer=', optimizer, '\\n')\n",
    "    model = make_NN('binary_crossentropy', optimizer, layers, input_dim=X_train.shape[1])\n",
    "    model.fit(X_train, y_train, epochs=30, batch_size=256, validation_data=(X_val, y_val))\n",
    "    y_pred = np.rint(model.predict(X_val)).astype(int)\n",
    "    print(classification_report(y_val, y_pred))\n",
    "    results.append([optimizer, layers, accuracy_score(y_val, y_pred)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(results, columns = ['optimizer', 'layers', 'accuracy'])\n",
    "res_df = res_df.sort_values('accuracy', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>optimizer</th>\n",
       "      <th>layers</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>DS_256 DS_128 BN DO DS_64 DS_32 BN DO DS_16 DS_8</td>\n",
       "      <td>0.786384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>DS_256 DS_128 BN DO DS_128 DS_64 BN DO DS_32 D...</td>\n",
       "      <td>0.786258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>DS_512 DS_256 BN DO DS_128 DS_64 BN DO DS_32 D...</td>\n",
       "      <td>0.781215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  optimizer                                             layers  accuracy\n",
       "0    Adamax   DS_256 DS_128 BN DO DS_64 DS_32 BN DO DS_16 DS_8  0.786384\n",
       "2    Adamax  DS_256 DS_128 BN DO DS_128 DS_64 BN DO DS_32 D...  0.786258\n",
       "1    Adamax  DS_512 DS_256 BN DO DS_128 DS_64 BN DO DS_32 D...  0.781215"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr= 0.001\n",
      "decay= 0.0 \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 4s 41us/step - loss: 0.5218 - accuracy: 0.7517 - val_loss: 0.5032 - val_accuracy: 0.7599\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4627 - accuracy: 0.7783 - val_loss: 0.4605 - val_accuracy: 0.7752\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4468 - accuracy: 0.7797 - val_loss: 0.4501 - val_accuracy: 0.7783\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 2s 24us/step - loss: 0.4375 - accuracy: 0.7819 - val_loss: 0.4408 - val_accuracy: 0.7779\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4315 - accuracy: 0.7838 - val_loss: 0.4309 - val_accuracy: 0.7822\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4253 - accuracy: 0.7840 - val_loss: 0.4254 - val_accuracy: 0.7823\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4208 - accuracy: 0.7858 - val_loss: 0.4235 - val_accuracy: 0.7803\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4178 - accuracy: 0.7853 - val_loss: 0.4193 - val_accuracy: 0.7829\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4139 - accuracy: 0.7886 - val_loss: 0.4215 - val_accuracy: 0.7826\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 2s 24us/step - loss: 0.4112 - accuracy: 0.7904 - val_loss: 0.4151 - val_accuracy: 0.7856\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 2s 24us/step - loss: 0.4081 - accuracy: 0.7937 - val_loss: 0.4160 - val_accuracy: 0.7844\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4055 - accuracy: 0.7937 - val_loss: 0.4121 - val_accuracy: 0.7860\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4016 - accuracy: 0.7980 - val_loss: 0.4118 - val_accuracy: 0.7857\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3998 - accuracy: 0.7980 - val_loss: 0.4106 - val_accuracy: 0.7856\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.3978 - accuracy: 0.7988 - val_loss: 0.4091 - val_accuracy: 0.7866\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.76      0.75     10216\n",
      "           1       0.82      0.80      0.81     13579\n",
      "\n",
      "    accuracy                           0.79     23795\n",
      "   macro avg       0.78      0.78      0.78     23795\n",
      "weighted avg       0.79      0.79      0.79     23795\n",
      "\n",
      "lr= 0.001\n",
      "decay= 0.01 \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 4s 43us/step - loss: 0.5370 - accuracy: 0.7651 - val_loss: 0.5398 - val_accuracy: 0.7607\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4931 - accuracy: 0.7779 - val_loss: 0.4890 - val_accuracy: 0.7766\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 2s 24us/step - loss: 0.4823 - accuracy: 0.7807 - val_loss: 0.4828 - val_accuracy: 0.7760\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4770 - accuracy: 0.7814 - val_loss: 0.4790 - val_accuracy: 0.7749\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4741 - accuracy: 0.7805 - val_loss: 0.4761 - val_accuracy: 0.7764\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 2s 24us/step - loss: 0.4714 - accuracy: 0.7808 - val_loss: 0.4740 - val_accuracy: 0.7761\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4695 - accuracy: 0.7815 - val_loss: 0.4722 - val_accuracy: 0.7770\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 2s 24us/step - loss: 0.4680 - accuracy: 0.7818 - val_loss: 0.4708 - val_accuracy: 0.7768\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.4669 - accuracy: 0.7810 - val_loss: 0.4696 - val_accuracy: 0.7772\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4658 - accuracy: 0.7820 - val_loss: 0.4685 - val_accuracy: 0.7774\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4647 - accuracy: 0.7813 - val_loss: 0.4676 - val_accuracy: 0.7772\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4639 - accuracy: 0.7811 - val_loss: 0.4668 - val_accuracy: 0.7768\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4633 - accuracy: 0.7815 - val_loss: 0.4661 - val_accuracy: 0.7772\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4622 - accuracy: 0.7820 - val_loss: 0.4654 - val_accuracy: 0.7772\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4618 - accuracy: 0.7830 - val_loss: 0.4648 - val_accuracy: 0.7776\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.82      0.76     10216\n",
      "           1       0.85      0.74      0.79     13579\n",
      "\n",
      "    accuracy                           0.78     23795\n",
      "   macro avg       0.78      0.78      0.78     23795\n",
      "weighted avg       0.79      0.78      0.78     23795\n",
      "\n",
      "lr= 0.003\n",
      "decay= 0.0 \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 4s 42us/step - loss: 0.5278 - accuracy: 0.7595 - val_loss: 0.4907 - val_accuracy: 0.7716\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4600 - accuracy: 0.7794 - val_loss: 0.4497 - val_accuracy: 0.7745\n",
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4367 - accuracy: 0.7794 - val_loss: 0.4346 - val_accuracy: 0.7735\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4237 - accuracy: 0.7839 - val_loss: 0.4228 - val_accuracy: 0.7810\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4154 - accuracy: 0.7871 - val_loss: 0.4181 - val_accuracy: 0.7804\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4096 - accuracy: 0.7901 - val_loss: 0.4130 - val_accuracy: 0.7846\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4052 - accuracy: 0.7933 - val_loss: 0.4155 - val_accuracy: 0.7864\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4005 - accuracy: 0.7954 - val_loss: 0.4099 - val_accuracy: 0.7873\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3965 - accuracy: 0.7986 - val_loss: 0.4083 - val_accuracy: 0.7879\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3933 - accuracy: 0.8010 - val_loss: 0.4116 - val_accuracy: 0.7845\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3906 - accuracy: 0.8031 - val_loss: 0.4058 - val_accuracy: 0.7892\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.3866 - accuracy: 0.8048 - val_loss: 0.4057 - val_accuracy: 0.7898\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 2s 24us/step - loss: 0.3830 - accuracy: 0.8071 - val_loss: 0.4055 - val_accuracy: 0.7888\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 2s 24us/step - loss: 0.3806 - accuracy: 0.8090 - val_loss: 0.4068 - val_accuracy: 0.7883\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 2s 24us/step - loss: 0.3788 - accuracy: 0.8096 - val_loss: 0.4041 - val_accuracy: 0.7888\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.76      0.76     10216\n",
      "           1       0.82      0.81      0.81     13579\n",
      "\n",
      "    accuracy                           0.79     23795\n",
      "   macro avg       0.78      0.79      0.78     23795\n",
      "weighted avg       0.79      0.79      0.79     23795\n",
      "\n",
      "lr= 0.003\n",
      "decay= 0.01 \n",
      "\n",
      "Train on 95178 samples, validate on 23795 samples\n",
      "Epoch 1/15\n",
      "95178/95178 [==============================] - 4s 43us/step - loss: 0.4932 - accuracy: 0.7705 - val_loss: 0.4948 - val_accuracy: 0.7630\n",
      "Epoch 2/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4581 - accuracy: 0.7807 - val_loss: 0.4546 - val_accuracy: 0.7787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4497 - accuracy: 0.7814 - val_loss: 0.4496 - val_accuracy: 0.7785\n",
      "Epoch 4/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4464 - accuracy: 0.7817 - val_loss: 0.4462 - val_accuracy: 0.7806\n",
      "Epoch 5/15\n",
      "95178/95178 [==============================] - 2s 24us/step - loss: 0.4435 - accuracy: 0.7832 - val_loss: 0.4446 - val_accuracy: 0.7802\n",
      "Epoch 6/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4425 - accuracy: 0.7827 - val_loss: 0.4432 - val_accuracy: 0.7796\n",
      "Epoch 7/15\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.4408 - accuracy: 0.7836 - val_loss: 0.4420 - val_accuracy: 0.7799\n",
      "Epoch 8/15\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.4398 - accuracy: 0.7834 - val_loss: 0.4416 - val_accuracy: 0.7788\n",
      "Epoch 9/15\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.4384 - accuracy: 0.7846 - val_loss: 0.4406 - val_accuracy: 0.7797\n",
      "Epoch 10/15\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.4379 - accuracy: 0.7844 - val_loss: 0.4399 - val_accuracy: 0.7799\n",
      "Epoch 11/15\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.4371 - accuracy: 0.7846 - val_loss: 0.4396 - val_accuracy: 0.7800\n",
      "Epoch 12/15\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.4368 - accuracy: 0.7854 - val_loss: 0.4391 - val_accuracy: 0.7798\n",
      "Epoch 13/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4362 - accuracy: 0.7845 - val_loss: 0.4387 - val_accuracy: 0.7805\n",
      "Epoch 14/15\n",
      "95178/95178 [==============================] - 2s 23us/step - loss: 0.4355 - accuracy: 0.7853 - val_loss: 0.4383 - val_accuracy: 0.7803\n",
      "Epoch 15/15\n",
      "95178/95178 [==============================] - 2s 22us/step - loss: 0.4353 - accuracy: 0.7853 - val_loss: 0.4383 - val_accuracy: 0.7800\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.83      0.76     10216\n",
      "           1       0.85      0.75      0.79     13579\n",
      "\n",
      "    accuracy                           0.78     23795\n",
      "   macro avg       0.78      0.79      0.78     23795\n",
      "weighted avg       0.79      0.78      0.78     23795\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for lr in [0.001, 0.003]:\n",
    "  for decay in [0.0, 0.01]:\n",
    "    print('lr=', lr)\n",
    "    print('decay=', decay, '\\n')\n",
    "    model = make_NN('binary_crossentropy', keras.optimizers.Adamax(lr=lr, decay=decay), 'DS_256 DS_128 BN DO DS_64 DS_32 BN DO DS_16 DS_8', input_dim=X_train.shape[1])\n",
    "    model.fit(X_train, y_train, epochs=15, batch_size=256, validation_data=(X_val, y_val))\n",
    "    y_pred = np.rint(model.predict(X_val)).astype(int)\n",
    "    print(classification_report(y_val, y_pred))\n",
    "    results.append([lr, decay, accuracy_score(y_val, y_pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(results, columns = ['learning_rate', 'decay', 'accuracy'])\n",
    "res_df = res_df.sort_values('accuracy', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>decay</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.788821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.786594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.779996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.777558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   learning_rate  decay  accuracy\n",
       "2          0.003   0.00  0.788821\n",
       "0          0.001   0.00  0.786594\n",
       "3          0.003   0.01  0.779996\n",
       "1          0.001   0.01  0.777558"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
